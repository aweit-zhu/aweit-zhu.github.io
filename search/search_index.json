{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. mkdocs gh-deploy --force - \u90e8\u7f72\u81f3 GITHUB PAGE Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"About"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. mkdocs gh-deploy --force - \u90e8\u7f72\u81f3 GITHUB PAGE","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"dapr/","text":"Dapr \u5982\u4f55\u5728 Ubuntu \u5b89\u88dd Java 11 sudo apt-get update sudo apt-get install openjdk-11-jdk Deploy Dapr on K8S \u53c3\u8003 https://docs.dapr.io/operations/hosting/kubernetes/kubernetes-deploy/ \u9810\u8a2d dapr-dashboard \u6c92\u6709\u5c0d\u5916\u958b\u7aef\u53e3\uff0c\u6240\u4ee5\u8981\u5728\u672c\u6a5f\u57f7\u884c kubectl port-forward \u5982\u4f55\u5728\u672c\u6a5f\u4f7f\u7528 kubectl port-forward\uff0c\u4ee5 service/{myservice} \u70ba\u4f8b \u78ba\u8a8d C:\\Users\\user.kube \u6a94\u6848\u7684\u8cc7\u8a0a\uff0c\u70ba microk8s config \u7684\u8cc7\u8a0a \u57f7\u884c\u6307\u4ee4\uff1a kubectl port-forward service/dapr-dashboard 8080:8080 -n dapr-system \u6253\u958b\u672c\u6a5f\u700f\u89bd\u5668 http://localhost:8080","title":"Dapr"},{"location":"dapr/#dapr","text":"","title":"Dapr"},{"location":"dapr/#ubuntu-java-11","text":"sudo apt-get update sudo apt-get install openjdk-11-jdk","title":"\u5982\u4f55\u5728 Ubuntu \u5b89\u88dd Java 11"},{"location":"dapr/#deploy-dapr-on-k8s","text":"\u53c3\u8003 https://docs.dapr.io/operations/hosting/kubernetes/kubernetes-deploy/ \u9810\u8a2d dapr-dashboard \u6c92\u6709\u5c0d\u5916\u958b\u7aef\u53e3\uff0c\u6240\u4ee5\u8981\u5728\u672c\u6a5f\u57f7\u884c kubectl port-forward","title":"Deploy Dapr on K8S"},{"location":"dapr/#kubectl-port-forward-servicemyservice","text":"\u78ba\u8a8d C:\\Users\\user.kube \u6a94\u6848\u7684\u8cc7\u8a0a\uff0c\u70ba microk8s config \u7684\u8cc7\u8a0a \u57f7\u884c\u6307\u4ee4\uff1a kubectl port-forward service/dapr-dashboard 8080:8080 -n dapr-system \u6253\u958b\u672c\u6a5f\u700f\u89bd\u5668 http://localhost:8080","title":"\u5982\u4f55\u5728\u672c\u6a5f\u4f7f\u7528 kubectl port-forward\uff0c\u4ee5 service/{myservice} \u70ba\u4f8b"},{"location":"docker/","text":"\u4f7f\u7528 Docker \u7684 Volume \u4f86\u90e8\u7f72 war \u6a94 Dockerfile FROM openjdk:8 RUN apt-get update && apt-get install -y wget RUN cd / RUN wget https://dlcdn.apache.org/tomcat/tomcat-10/v10.1.10/bin/apache-tomcat-10.1.10.tar.gz RUN tar zxvf apache-tomcat-10.1.10.tar.gz VOLUME [\"/apache-tomcat-10.1.10/webapps\"] CMD [\"/apache-tomcat-10.1.10/bin/catalina.sh\", \"run\"] \u900f\u904e VSCode \u57f7\u884c docker build \u57f7\u884c docker run -d --name tomcat -p 8080:8080 tomcatserver \u5377\u5b97\u6709\u5169\u7a2e\u639b\u8f09\u65b9\u5f0f\uff0c\u4e00\u7a2e\u5377\u5b97\u639b\u8f09\u3001\u4e00\u7a2e\u7e6b\u7d50\u639b\u8f09\u3002 docker inspect -f '{{.Mounts}}' tomcat [{volume 64c8c0866ae8f36f7471359d1d0c564b6fc3088fec5137a5f21b51c0d145cbb2 /var/lib/docker/volumes/64c8c0866ae8f36f7471359d1d0c564b6fc3088fec5137a5f21b51c0d145cbb2/_data /apache-tomcat-10.1.10/webapps local true }] \u5377\u5b97\u639b\u8f09\uff1a(\u9019\u6b21\u4f8b\u5b50\u9069\u7528) \u7e6b\u7d50\u639b\u8f09\uff1a(\u4e00\u822c\u63a8\u85a6\u7528\u9019\u7a2e) \u6bd4\u8f03\u5169\u8005\u4e0d\u540c\uff1a Volume \u5b58\u653e\u5728\u4e3b\u6a5f\u6a94\u6848\u7cfb\u7d71\u4e2d\u7531 Docker \u7ba1\u7406\u7684\u5730\u65b9\uff0c\u5728 Linux \u4f5c\u696d\u7cfb\u7d71\u662f /var/lib/docker/volumes/ \u6b64\u8def\u5f91\u3002\u975e Docker \u7684\u884c\u7a0b\u4e0d\u61c9\u8a72\u4fee\u6539\u6a94\u6848\u7cfb\u7d71\u4e2d\u7684\u9019\u4e00\u90e8\u5206\u3002\u8981\u5728 Docker \u4e2d\u7559\u5b58\u8cc7\u6599\uff0cvolumes \u662f\u6700\u597d\u7684\u65b9\u6cd5\u3002 Bind mount \u53ef\u5b58\u653e\u5728\u4e3b\u6a5f\u6a94\u6848\u7cfb\u7d71\u4e2d\u7684\u4efb\u4f55\u5730\u65b9\uff0c\u975e Docker \u884c\u7a0b\u6216 Docker \u5bb9\u5668\u53ef\u96a8\u6642\u4fee\u6539\u3002 tmpfs mount \u53ea\u5b58\u653e\u5728\u4e3b\u6a5f\u7684\u8a18\u61b6\u9ad4\u4e2d\uff0c\u4e0d\u6703\u5beb\u5165\u4e3b\u6a5f\u7684\u6a94\u6848\u7cfb\u7d71\u3002 https://ithelp.ithome.com.tw/articles/10207973","title":"Docker"},{"location":"docker/#docker-volume-war","text":"Dockerfile FROM openjdk:8 RUN apt-get update && apt-get install -y wget RUN cd / RUN wget https://dlcdn.apache.org/tomcat/tomcat-10/v10.1.10/bin/apache-tomcat-10.1.10.tar.gz RUN tar zxvf apache-tomcat-10.1.10.tar.gz VOLUME [\"/apache-tomcat-10.1.10/webapps\"] CMD [\"/apache-tomcat-10.1.10/bin/catalina.sh\", \"run\"] \u900f\u904e VSCode \u57f7\u884c docker build \u57f7\u884c docker run -d --name tomcat -p 8080:8080 tomcatserver \u5377\u5b97\u6709\u5169\u7a2e\u639b\u8f09\u65b9\u5f0f\uff0c\u4e00\u7a2e\u5377\u5b97\u639b\u8f09\u3001\u4e00\u7a2e\u7e6b\u7d50\u639b\u8f09\u3002 docker inspect -f '{{.Mounts}}' tomcat [{volume 64c8c0866ae8f36f7471359d1d0c564b6fc3088fec5137a5f21b51c0d145cbb2 /var/lib/docker/volumes/64c8c0866ae8f36f7471359d1d0c564b6fc3088fec5137a5f21b51c0d145cbb2/_data /apache-tomcat-10.1.10/webapps local true }] \u5377\u5b97\u639b\u8f09\uff1a(\u9019\u6b21\u4f8b\u5b50\u9069\u7528) \u7e6b\u7d50\u639b\u8f09\uff1a(\u4e00\u822c\u63a8\u85a6\u7528\u9019\u7a2e) \u6bd4\u8f03\u5169\u8005\u4e0d\u540c\uff1a Volume \u5b58\u653e\u5728\u4e3b\u6a5f\u6a94\u6848\u7cfb\u7d71\u4e2d\u7531 Docker \u7ba1\u7406\u7684\u5730\u65b9\uff0c\u5728 Linux \u4f5c\u696d\u7cfb\u7d71\u662f /var/lib/docker/volumes/ \u6b64\u8def\u5f91\u3002\u975e Docker \u7684\u884c\u7a0b\u4e0d\u61c9\u8a72\u4fee\u6539\u6a94\u6848\u7cfb\u7d71\u4e2d\u7684\u9019\u4e00\u90e8\u5206\u3002\u8981\u5728 Docker \u4e2d\u7559\u5b58\u8cc7\u6599\uff0cvolumes \u662f\u6700\u597d\u7684\u65b9\u6cd5\u3002 Bind mount \u53ef\u5b58\u653e\u5728\u4e3b\u6a5f\u6a94\u6848\u7cfb\u7d71\u4e2d\u7684\u4efb\u4f55\u5730\u65b9\uff0c\u975e Docker \u884c\u7a0b\u6216 Docker \u5bb9\u5668\u53ef\u96a8\u6642\u4fee\u6539\u3002 tmpfs mount \u53ea\u5b58\u653e\u5728\u4e3b\u6a5f\u7684\u8a18\u61b6\u9ad4\u4e2d\uff0c\u4e0d\u6703\u5beb\u5165\u4e3b\u6a5f\u7684\u6a94\u6848\u7cfb\u7d71\u3002 https://ithelp.ithome.com.tw/articles/10207973","title":"\u4f7f\u7528 Docker \u7684 Volume \u4f86\u90e8\u7f72 war \u6a94"},{"location":"drone/","text":"Drone \u7b46\u8a18 1.\u900f\u904e ngrok\uff0c\u5efa\u7acb\u516c\u958bIP\uff0c\u7d81\u5b9a localhost:8082 (\u91cd\u555f\u3001\u91cd\u958b\u6a5f\u5c31\u6c92\u4e86) 2.\u7d81\u5b9agithub \u524d\u5f80 settings -> Developer Settings \u5efa\u7acb\u4e00\u500b OAuth APP Homepage URL\uff1a\u586b\u525b\u525b\u7528 ngrok \u63d0\u5171\u7684 \u516c\u958b IP https://b5c9-58-115-111-122.ngrok-free.app \u7522\u751f Client ID \u8207 Client Secret Client ID\uff1a 3ab12bcea99814fd59df Client Secret\uff1a 2ae999f7e3e8bc29c1c5fec1918d1d1ba13a2067 3.\u900f\u904e Docker Compose \u5efa\u7acb drone server \u958b\u555f CMD\uff0c\u8f38\u5165 WSL\u3002 \u5207\u5230 home \u8def\u5f91\u3002 cd ~ \u5efa\u7acb drone-server \u8cc7\u6599\u593e mkdir drone-server cd drone-server \u5efa\u7acb docker-compose.github.yml nano docker-compose.github.yml \u8cbc\u4e0a\u4ee5\u4e0b version: '2' services: drone-server: /assets/image: drone/drone:1 ports: - 8082:80 volumes: - ./:/data restart: always environment: - DRONE_SERVER_HOST=${DRONE_SERVER_HOST} - DRONE_SERVER_PROTO=${DRONE_SERVER_PROTO} - DRONE_RPC_SECRET=${DRONE_RPC_SECRET} - DRONE_GITHUB_SERVER=<https://github.com> - DRONE_GITHUB_CLIENT_ID=${DRONE_GITHUB_CLIENT_ID} - DRONE_GITHUB_CLIENT_SECRET=${DRONE_GITHUB_CLIENT_SECRET} - DRONE_LOGS_PRETTY=true - DRONE_LOGS_COLOR=true - DRONE_USER_CREATE=username:aweit-zhu,admin:true runner for docker version drone-runner: /assets/image: drone/drone-runner-docker:1 restart: always depends_on: - drone-server volumes: - /var/run/docker.sock:/var/run/docker.sock environment: - DRONE_RPC_HOST=${DRONE_RPC_HOST} - DRONE_RPC_PROTO=${DRONE_RPC_PROTO} - DRONE_RPC_SECRET=${DRONE_RPC_SECRET} - DRONE_RUNNER_CAPACITY=3 \u5efa\u7acb .env nano .env \u8cbc\u4e0a\u4ee5\u4e0b \u6211\u5011\u76ee\u524d\u6709\u7684\u8cc7\u8a0a\u70ba\uff1a \u516c\u958b IP <https://b5c9-58-115-111-122.ngrok-free.app> Client ID\uff1a 3ab12bcea99814fd59df Client Secret\uff1a 2ae999f7e3e8bc29c1c5fec1918d1d1ba13a2067 DRONE_SERVER_HOST=<https://b5c9-58-115-111-122.ngrok-free.app> DRONE_SERVER_PROTO=https DRONE_RPC_SECRET=123 DRONE_RPC_HOST=drone-server DRONE_RPC_PROTO=http DRONE_GITHUB_CLIENT_ID=3ab12bcea99814fd59df DRONE_GITHUB_CLIENT_SECRET=2ae999f7e3e8bc29c1c5fec1918d1d1ba13a2067 \u555f\u52d5\u5bb9\u5668 docker-compose -f docker-compose.github.yml up -d \u6253\u958b https://b5c9-58-115-111-122.ngrok-free.app \u9ede\u9078 Visit Site \u9ede\u9078 Authorize aweit-zhu Drone Server \u7684\u64cd\u4f5c \u9078\u64c7 aweit-zhu/docker-demo \u9032\u884c Activate \u9078\u64c7 Trusted \u9ede\u9078 Save \u56de\u5230 github \u505a\u8a2d\u5b9a \u5c07 Payload URL \u4fee\u6539\u70ba \u4fee\u6539\u70ba https://b5c9-58-115-111-122.ngrok-free.app /hook 4. \u64b0\u5beb .drone.yml \u6a94 \u5efa\u7acb ssh_password_master secret \u5efa\u7acb ssh_password secret \u89e3\u6790 .drone.yml (\u7565) kind: pipeline type: docker name: docker-demo-drone steps: - name: test /assets/image: maven:3-jdk-8 volumes: - name: maven-cache path: /root/.m2 - name: maven-build path: /app/build commands: - mvn install -DskipTests=true -Dmaven.javadoc.skip=true -B -V - mvn test -B - name: package /assets/image: maven:3-jdk-8 volumes: - name: maven-cache path: /root/.m2 - name: maven-build path: /app/build commands: - mvn clean package - cp target/docker-demo.jar /app/build/docker-demo.jar - cp Dockerfile /app/build/Dockerfile - cp docker-k8s-demo-deployment.yaml /app/build/docker-k8s-demo-deployment.yaml - cp deploymentservice.yaml /app/build/deploymentservice.yaml - cp run.sh /app/build/run.sh - name: scp files /assets/image: appleboy/drone-scp settings: host: 192.168.0.17 username: root password: from_secret: ssh_password_master port: 22 command_timeout: 2m target: /mydata/maven/build source: ./* - name: build-start01 /assets/image: appleboy/drone-ssh settings: host: 172.31.93.122 username: root password: from_secret: ssh_password port: 2222 command_timeout: 5m script: - cd /mydata/maven/build - chmod +x run.sh - ./run.sh - name: build-start02 /assets/image: appleboy/drone-ssh settings: host: 192.168.0.17 username: vboxuser password: from_secret: ssh_password_master port: 22 command_timeout: 5m script: - cd /mydata/maven/build - microk8s.kubectl delete deploy docker-k8s-demo-deployment - microk8s.kubectl apply -f ./docker-k8s-demo-deployment.yaml volumes: - name: maven-build host: path: /mydata/maven/build - name: maven-cache host: path: /mydata/maven/cache \u53ea\u8981 push \u5230\u9060\u7aef\uff0c\u5c31\u6703\u89f8\u767c drone \u57f7\u884c\u3002 \u9a57\u8b49 \u6253\u958b\u700f\u89bd\u5668\uff0c\u8f38\u5165URL\uff1a http://192.168.0.17:60000/","title":"Drone"},{"location":"drone/#drone","text":"","title":"Drone \u7b46\u8a18"},{"location":"drone/#1-ngrokip-localhost8082","text":"","title":"1.\u900f\u904e ngrok\uff0c\u5efa\u7acb\u516c\u958bIP\uff0c\u7d81\u5b9a localhost:8082  (\u91cd\u555f\u3001\u91cd\u958b\u6a5f\u5c31\u6c92\u4e86)"},{"location":"drone/#2github","text":"\u524d\u5f80 settings -> Developer Settings \u5efa\u7acb\u4e00\u500b OAuth APP Homepage URL\uff1a\u586b\u525b\u525b\u7528 ngrok \u63d0\u5171\u7684 \u516c\u958b IP https://b5c9-58-115-111-122.ngrok-free.app \u7522\u751f Client ID \u8207 Client Secret Client ID\uff1a 3ab12bcea99814fd59df Client Secret\uff1a 2ae999f7e3e8bc29c1c5fec1918d1d1ba13a2067","title":"2.\u7d81\u5b9agithub"},{"location":"drone/#3-docker-compose-drone-server","text":"\u958b\u555f CMD\uff0c\u8f38\u5165 WSL\u3002 \u5207\u5230 home \u8def\u5f91\u3002 cd ~ \u5efa\u7acb drone-server \u8cc7\u6599\u593e mkdir drone-server cd drone-server \u5efa\u7acb docker-compose.github.yml nano docker-compose.github.yml \u8cbc\u4e0a\u4ee5\u4e0b version: '2' services: drone-server: /assets/image: drone/drone:1 ports: - 8082:80 volumes: - ./:/data restart: always environment: - DRONE_SERVER_HOST=${DRONE_SERVER_HOST} - DRONE_SERVER_PROTO=${DRONE_SERVER_PROTO} - DRONE_RPC_SECRET=${DRONE_RPC_SECRET} - DRONE_GITHUB_SERVER=<https://github.com> - DRONE_GITHUB_CLIENT_ID=${DRONE_GITHUB_CLIENT_ID} - DRONE_GITHUB_CLIENT_SECRET=${DRONE_GITHUB_CLIENT_SECRET} - DRONE_LOGS_PRETTY=true - DRONE_LOGS_COLOR=true - DRONE_USER_CREATE=username:aweit-zhu,admin:true","title":"3.\u900f\u904e Docker Compose \u5efa\u7acb drone server"},{"location":"drone/#runner-for-docker-version","text":"drone-runner: /assets/image: drone/drone-runner-docker:1 restart: always depends_on: - drone-server volumes: - /var/run/docker.sock:/var/run/docker.sock environment: - DRONE_RPC_HOST=${DRONE_RPC_HOST} - DRONE_RPC_PROTO=${DRONE_RPC_PROTO} - DRONE_RPC_SECRET=${DRONE_RPC_SECRET} - DRONE_RUNNER_CAPACITY=3 \u5efa\u7acb .env nano .env \u8cbc\u4e0a\u4ee5\u4e0b \u6211\u5011\u76ee\u524d\u6709\u7684\u8cc7\u8a0a\u70ba\uff1a \u516c\u958b IP <https://b5c9-58-115-111-122.ngrok-free.app> Client ID\uff1a 3ab12bcea99814fd59df Client Secret\uff1a 2ae999f7e3e8bc29c1c5fec1918d1d1ba13a2067 DRONE_SERVER_HOST=<https://b5c9-58-115-111-122.ngrok-free.app> DRONE_SERVER_PROTO=https DRONE_RPC_SECRET=123 DRONE_RPC_HOST=drone-server DRONE_RPC_PROTO=http DRONE_GITHUB_CLIENT_ID=3ab12bcea99814fd59df DRONE_GITHUB_CLIENT_SECRET=2ae999f7e3e8bc29c1c5fec1918d1d1ba13a2067 \u555f\u52d5\u5bb9\u5668 docker-compose -f docker-compose.github.yml up -d \u6253\u958b https://b5c9-58-115-111-122.ngrok-free.app \u9ede\u9078 Visit Site \u9ede\u9078 Authorize aweit-zhu Drone Server \u7684\u64cd\u4f5c \u9078\u64c7 aweit-zhu/docker-demo \u9032\u884c Activate \u9078\u64c7 Trusted \u9ede\u9078 Save \u56de\u5230 github \u505a\u8a2d\u5b9a \u5c07 Payload URL \u4fee\u6539\u70ba \u4fee\u6539\u70ba https://b5c9-58-115-111-122.ngrok-free.app /hook","title":"runner for docker version"},{"location":"drone/#4-droneyml","text":"\u5efa\u7acb ssh_password_master secret \u5efa\u7acb ssh_password secret \u89e3\u6790 .drone.yml (\u7565) kind: pipeline type: docker name: docker-demo-drone steps: - name: test /assets/image: maven:3-jdk-8 volumes: - name: maven-cache path: /root/.m2 - name: maven-build path: /app/build commands: - mvn install -DskipTests=true -Dmaven.javadoc.skip=true -B -V - mvn test -B - name: package /assets/image: maven:3-jdk-8 volumes: - name: maven-cache path: /root/.m2 - name: maven-build path: /app/build commands: - mvn clean package - cp target/docker-demo.jar /app/build/docker-demo.jar - cp Dockerfile /app/build/Dockerfile - cp docker-k8s-demo-deployment.yaml /app/build/docker-k8s-demo-deployment.yaml - cp deploymentservice.yaml /app/build/deploymentservice.yaml - cp run.sh /app/build/run.sh - name: scp files /assets/image: appleboy/drone-scp settings: host: 192.168.0.17 username: root password: from_secret: ssh_password_master port: 22 command_timeout: 2m target: /mydata/maven/build source: ./* - name: build-start01 /assets/image: appleboy/drone-ssh settings: host: 172.31.93.122 username: root password: from_secret: ssh_password port: 2222 command_timeout: 5m script: - cd /mydata/maven/build - chmod +x run.sh - ./run.sh - name: build-start02 /assets/image: appleboy/drone-ssh settings: host: 192.168.0.17 username: vboxuser password: from_secret: ssh_password_master port: 22 command_timeout: 5m script: - cd /mydata/maven/build - microk8s.kubectl delete deploy docker-k8s-demo-deployment - microk8s.kubectl apply -f ./docker-k8s-demo-deployment.yaml volumes: - name: maven-build host: path: /mydata/maven/build - name: maven-cache host: path: /mydata/maven/cache \u53ea\u8981 push \u5230\u9060\u7aef\uff0c\u5c31\u6703\u89f8\u767c drone \u57f7\u884c\u3002 \u9a57\u8b49 \u6253\u958b\u700f\u89bd\u5668\uff0c\u8f38\u5165URL\uff1a http://192.168.0.17:60000/","title":"4. \u64b0\u5beb .drone.yml \u6a94"},{"location":"flutter/","text":"FlutterFire Step0. create a new Flutter app flutter create example Step1. Install the required command line tools install the Firebase CLI npm install -g firebase-tools \u767b\u5165 Firebase. firebase login \u5982\u679c\u5df2\u767b\u5165\uff0c\u6703\u986f\u793a Already logged in as aa4192696@gmail.com \u5982\u679c\u672a\u767b\u5165\uff0c\u5247\u6703\u8df3\u51fa\u767b\u5165\u8996\u7a97\uff0c\u8b93\u60a8\u900f\u904e Google \u5e33\u865f\u767b\u5165\u3002 ( https://console.firebase.google.com/ ) 3. \u5b89\u88dd FlutterFire CLI.(\u5168\u57df) dart pub global activate flutterfire_cli Step 2: Configure your apps to use Firebase \u9032\u5165\u5230 Flutter \u5c08\u6848\u6839\u76ee\u9304\uff0c\u57f7\u884c\u4ee5\u4e0b\u6307\u4ee4 flutterfire configure (1) \u9019\u908a\u4ee5\u5efa\u7acb\u65b0\u7684 firebase \u5c08\u6848\u70ba\u4f8b\uff0c\u540d\u5b57\u53d6\uff1a aweit-example (2) \u5168\u90e8\u5c31\u5c0d\u4e86\uff0c\u9019\u6a23\u6703\u5728 Firebase \u5c08\u6848\u4e2d\uff0c\u5efa\u7acb\u56db\u500b\u61c9\u7528\u7a0b\u5f0f\u3002 (3) \u90fd\u6210\u529f\u5f8c\uff0c\u6703\u986f\u793a\u4ee5\u4e0b\u8cc7\u8a0a\u3002 Step 3: Initialize Firebase in your app \u5b89\u88dd firebase_core flutter pub add firebase_core \u518d\u6b21\u57f7\u884c flutterfire configure \uff0c\u4ee5\u78ba\u8a8d Flrebase \u8a2d\u5b9a\u662f\u6700\u65b0\u7684\u3002 \u6253\u958b lib/main.dart\uff0c\u63d2\u5165\u4ee5\u4e0b\u4ee3\u78bc \u539f\u672c ``` import 'package:flutter/material.dart'; void main() { runApp(const MyApp()); } ``` \u8b8a\u66f4\u70ba ``` import 'package:flutter/material.dart'; import 'package:firebase_core/firebase_core.dart'; import 'firebase_options.dart'; Future main() async { WidgetsFlutterBinding.ensureInitialized(); await Firebase.initializeApp( options: DefaultFirebaseOptions.currentPlatform, ); runApp(const MyApp()); } ``` 4. flutter run \u76f8\u95dc\u932f\u8aa4\u8a0a\u606f\u8655\u7406\u65b9\u5f0f (1) \u82e5\u51fa\u73fe If ServicesBinding is a custom binding mixin, there must also be a custom binding class, like WidgetsFlutterBinding, but that mixes in the selected binding, and that is the class that must be constructed before using the \"instance\" getter \u8981\u65b0\u589e\u4e00\u884c\u7a0b\u5f0f\u78bc\u5728 lib/main.dart \u7684 main \u65b9\u6cd5\u4e2d\u7684\u7b2c\u4e00\u884c\u3002 WidgetsFlutterBinding.ensureInitialized(); Step 4: Add Firebase plugins You access Firebase in your Flutter app through the various Firebase Flutter plugins, one for each Firebase product (for example: Cloud Firestore, Authentication, Analytics, etc.). Since Flutter is a multi-platform framework, each Firebase plugin is applicable for Apple, Android, and web platforms. So, if you add any Firebase plugin to your Flutter app, it will be used by the Apple, Android, and web versions of your app. flutter pub add {PLUGIN_NAME} flutterfire configure flutter run Get Started with Firebase Authentication on Flutter Google Enable Android - SHA Key keytool -list -v -alias androiddebugkey -keystore .\\.android\\debug.keystore password: android SHA1: 29:C4:07:39:3C:33:37:25:06:73:34:D4:DC:11:D2:7B:6C:34:F8:B4 SHA256: >56:F3:29:12:BB:90:C9:D9:95:0F:16:80:F1:19:EE:D2:99:CB:6B:95:4D:30:5D:16:F6:43:FB:72:31:97:54:36 Flutter Project \u76f8\u95dc\u8a2d\u5b9a \u5b89\u88dd\u5957\u4ef6\u3001\u8a2d\u5b9a Firebase flutter pub add firebase_auth flutter pub add google_sign_in flutterfire configure \u4fee\u6539\u8a2d\u5b9a\u503c android\\app\\build.gradle defaultConfig { multiDexEnabled true minSdkVersion 19 } Demo \u7a0b\u5f0f home.dart import 'package:flutter/material.dart'; import 'package:google_sign_in/google_sign_in.dart'; class HomePage extends StatefulWidget { @override _HomePageState createState() => _HomePageState(); } class _HomePageState extends State<HomePage> { bool _isLoggedIn = false; late GoogleSignInAccount _userObj; final GoogleSignIn _googleSignIn = GoogleSignIn(); @override Widget build(BuildContext context) { return Scaffold( appBar: AppBar(title: Text(\"Codesundar\")), body: Container( child: _isLoggedIn ? Column( children: [ Image.network(_userObj.photoUrl!), Text(_userObj.displayName!), Text(_userObj.email), TextButton( onPressed: () { _googleSignIn.signOut().then((value) { setState(() { _isLoggedIn = false; }); }).catchError((e) {}); }, child: Text(\"Logout\")) ], ) : Center( child: ElevatedButton( child: Text(\"Login with Google\"), onPressed: () { _googleSignIn.signIn().then((userData) { setState(() { _isLoggedIn = true; _userObj = userData!; }); }).catchError((e) { print(e); }); }, ), ), ), ); } } main.dart import 'package:firebase_auth/firebase_auth.dart'; import 'package:firebase_core/firebase_core.dart'; import 'package:flutter/material.dart'; import 'package:google_sign_in/google_sign_in.dart'; import 'firebase_options.dart'; import 'home.dart'; Future<void> main() async { WidgetsFlutterBinding.ensureInitialized(); await Firebase.initializeApp( options: DefaultFirebaseOptions.currentPlatform, ); runApp(const MyApp()); } class MyApp extends StatelessWidget { const MyApp({Key? key}) : super(key: key); @override Widget build(BuildContext context) { return MaterialApp( title: 'Google Authencation Demo', theme: ThemeData( primarySwatch: Colors.blue, ), home: HomePage(), ); } } \u756b\u9762 \u5982\u679c\u7528 Web \u6703\u7121\u6cd5\u505a\u51fa\u6548\u679c\uff0c\u51fa\u73fe\u4ee5\u4e0b\u932f\u8aa4\u8a0a\u606f\uff1a ClientID not set. Either set it on a <meta name=\\\"google-signin-client_id\\\" content=\\\"CLIENT_ID\\\" /> tag, or pass clientId when initializing GoogleSignIn (1) \u53bb\u627e Clitne id : 8073574655-9c9sa7onebk99ju1mf2i5e1kfi5m4hk0.apps.googleusercontent.com (2) \u4fee\u6539\u4ee3\u78bc home.dart \u539f\u672c final GoogleSignIn _googleSignIn = GoogleSignIn(); \u5f8c\u4f86 final GoogleSignIn _googleSignIn = GoogleSignIn(clientId: '8073574655-saga5315djjgbcgsgag5qfmgusl6tqpu.apps.googleusercontent.com'); (3) \u51fa\u73fe\u4ee5\u4e0b\u8a0a\u606f\uff0c\u5247\u76f4\u63a5\u9ede\u9078\uff0c\u7167\u8457\u6b65\u9a5f\u505a\u5373\u53ef\u3002 PlatformException(idpiframe_initialization_failed, Not a valid origin for the client: http://localhost:13529 has not been registered for client ID 8073574655-9c9sa7onebk99ju1mf2i5e1kfi5m4hk0.apps.googleusercontent.com. Please go to https://console.developers.google.com/ and register this origin for your project's client ID., https://developers.google.com/identity/sign-in/web/reference#error_codes, null) \u5df2\u6388\u6b0a\u7684 JavaScript \u4f86\u6e90\uff0c\u8981\u628a\u9019\u500bPort\u52a0\u4e0a\u53bb (4) Flutter run \u6642\uff0c\u6307\u5b9a PORT = 12621 \u53c3\u6578\u3002--web-port=12621 (5) \u518d\u6b21\u6253\u958b\u700f\u89bd\u5668\uff0c\u4e26\u4e14\u6e05\u9664\u5feb\u53d6\u548c\u700f\u89bd\u8cc7\u6599\u3002\u5c31\u53ef\u4ee5\u4e86\u3002 Try out an example app with Analytics flutter pub add firebase_analytics flutterfire configure Access the lib directory of the app, then delete the existing main.dart file. \u5c07\u4ee5\u4e0b\u5169\u500b\u6a94\u6848\uff0c\u8907\u88fd\u8cbc\u4e0a\u5230\u5c08\u6848\u4e2d lib/ https://github.com/firebase/flutterfire/blob/master/packages/firebase_analytics/firebase_analytics/example/lib/main.dart https://github.com/firebase/flutterfire/blob/master/packages/firebase_analytics/firebase_analytics/example/lib/tabs_page.dart \u57f7\u884c flutter run \u932f\u8aa4\u8a0a\u606f \u932f\u8aa41 \u932f\u8aa42 \u5982\u679c\u51fa\u73fe Missing google_app_id. Firebase Analytics disabled \u932f\u8aa4\u8a0a\u606f \u4ee3\u8868\u6709\u5169\u500b\u5730\u65b9\u9700\u8981\u4fee\u6539\uff1a - android\\build.gradle ``` classpath 'com.google.gms:google-services:4.3.15' ``` ![Alt text](image-61.png) - android\\app\\build.gradle ``` apply plugin: 'com.google.gms.google-services' ``` ![Alt text](image-62.png) \u6210\u679c","title":"FlutterFire"},{"location":"flutter/#flutterfire","text":"","title":"FlutterFire"},{"location":"flutter/#step0-create-a-new-flutter-app","text":"flutter create example","title":"Step0. create a new Flutter app"},{"location":"flutter/#step1-install-the-required-command-line-tools","text":"install the Firebase CLI npm install -g firebase-tools \u767b\u5165 Firebase. firebase login \u5982\u679c\u5df2\u767b\u5165\uff0c\u6703\u986f\u793a Already logged in as aa4192696@gmail.com \u5982\u679c\u672a\u767b\u5165\uff0c\u5247\u6703\u8df3\u51fa\u767b\u5165\u8996\u7a97\uff0c\u8b93\u60a8\u900f\u904e Google \u5e33\u865f\u767b\u5165\u3002 ( https://console.firebase.google.com/ ) 3. \u5b89\u88dd FlutterFire CLI.(\u5168\u57df) dart pub global activate flutterfire_cli","title":"Step1.  Install the required command line tools"},{"location":"flutter/#step-2-configure-your-apps-to-use-firebase","text":"\u9032\u5165\u5230 Flutter \u5c08\u6848\u6839\u76ee\u9304\uff0c\u57f7\u884c\u4ee5\u4e0b\u6307\u4ee4 flutterfire configure (1) \u9019\u908a\u4ee5\u5efa\u7acb\u65b0\u7684 firebase \u5c08\u6848\u70ba\u4f8b\uff0c\u540d\u5b57\u53d6\uff1a aweit-example (2) \u5168\u90e8\u5c31\u5c0d\u4e86\uff0c\u9019\u6a23\u6703\u5728 Firebase \u5c08\u6848\u4e2d\uff0c\u5efa\u7acb\u56db\u500b\u61c9\u7528\u7a0b\u5f0f\u3002 (3) \u90fd\u6210\u529f\u5f8c\uff0c\u6703\u986f\u793a\u4ee5\u4e0b\u8cc7\u8a0a\u3002","title":"Step 2: Configure your apps to use Firebase"},{"location":"flutter/#step-3-initialize-firebase-in-your-app","text":"\u5b89\u88dd firebase_core flutter pub add firebase_core \u518d\u6b21\u57f7\u884c flutterfire configure \uff0c\u4ee5\u78ba\u8a8d Flrebase \u8a2d\u5b9a\u662f\u6700\u65b0\u7684\u3002 \u6253\u958b lib/main.dart\uff0c\u63d2\u5165\u4ee5\u4e0b\u4ee3\u78bc \u539f\u672c ``` import 'package:flutter/material.dart'; void main() { runApp(const MyApp()); } ``` \u8b8a\u66f4\u70ba ``` import 'package:flutter/material.dart'; import 'package:firebase_core/firebase_core.dart'; import 'firebase_options.dart'; Future main() async { WidgetsFlutterBinding.ensureInitialized(); await Firebase.initializeApp( options: DefaultFirebaseOptions.currentPlatform, ); runApp(const MyApp()); } ``` 4. flutter run \u76f8\u95dc\u932f\u8aa4\u8a0a\u606f\u8655\u7406\u65b9\u5f0f (1) \u82e5\u51fa\u73fe If ServicesBinding is a custom binding mixin, there must also be a custom binding class, like WidgetsFlutterBinding, but that mixes in the selected binding, and that is the class that must be constructed before using the \"instance\" getter \u8981\u65b0\u589e\u4e00\u884c\u7a0b\u5f0f\u78bc\u5728 lib/main.dart \u7684 main \u65b9\u6cd5\u4e2d\u7684\u7b2c\u4e00\u884c\u3002 WidgetsFlutterBinding.ensureInitialized();","title":"Step 3: Initialize Firebase in your app"},{"location":"flutter/#step-4-add-firebase-plugins","text":"You access Firebase in your Flutter app through the various Firebase Flutter plugins, one for each Firebase product (for example: Cloud Firestore, Authentication, Analytics, etc.). Since Flutter is a multi-platform framework, each Firebase plugin is applicable for Apple, Android, and web platforms. So, if you add any Firebase plugin to your Flutter app, it will be used by the Apple, Android, and web versions of your app. flutter pub add {PLUGIN_NAME} flutterfire configure flutter run","title":"Step 4: Add Firebase plugins"},{"location":"flutter/#get-started-with-firebase-authentication-on-flutter","text":"Google Enable Android - SHA Key keytool -list -v -alias androiddebugkey -keystore .\\.android\\debug.keystore password: android SHA1: 29:C4:07:39:3C:33:37:25:06:73:34:D4:DC:11:D2:7B:6C:34:F8:B4 SHA256: >56:F3:29:12:BB:90:C9:D9:95:0F:16:80:F1:19:EE:D2:99:CB:6B:95:4D:30:5D:16:F6:43:FB:72:31:97:54:36 Flutter Project \u76f8\u95dc\u8a2d\u5b9a \u5b89\u88dd\u5957\u4ef6\u3001\u8a2d\u5b9a Firebase flutter pub add firebase_auth flutter pub add google_sign_in flutterfire configure \u4fee\u6539\u8a2d\u5b9a\u503c android\\app\\build.gradle defaultConfig { multiDexEnabled true minSdkVersion 19 } Demo \u7a0b\u5f0f home.dart import 'package:flutter/material.dart'; import 'package:google_sign_in/google_sign_in.dart'; class HomePage extends StatefulWidget { @override _HomePageState createState() => _HomePageState(); } class _HomePageState extends State<HomePage> { bool _isLoggedIn = false; late GoogleSignInAccount _userObj; final GoogleSignIn _googleSignIn = GoogleSignIn(); @override Widget build(BuildContext context) { return Scaffold( appBar: AppBar(title: Text(\"Codesundar\")), body: Container( child: _isLoggedIn ? Column( children: [ Image.network(_userObj.photoUrl!), Text(_userObj.displayName!), Text(_userObj.email), TextButton( onPressed: () { _googleSignIn.signOut().then((value) { setState(() { _isLoggedIn = false; }); }).catchError((e) {}); }, child: Text(\"Logout\")) ], ) : Center( child: ElevatedButton( child: Text(\"Login with Google\"), onPressed: () { _googleSignIn.signIn().then((userData) { setState(() { _isLoggedIn = true; _userObj = userData!; }); }).catchError((e) { print(e); }); }, ), ), ), ); } } main.dart import 'package:firebase_auth/firebase_auth.dart'; import 'package:firebase_core/firebase_core.dart'; import 'package:flutter/material.dart'; import 'package:google_sign_in/google_sign_in.dart'; import 'firebase_options.dart'; import 'home.dart'; Future<void> main() async { WidgetsFlutterBinding.ensureInitialized(); await Firebase.initializeApp( options: DefaultFirebaseOptions.currentPlatform, ); runApp(const MyApp()); } class MyApp extends StatelessWidget { const MyApp({Key? key}) : super(key: key); @override Widget build(BuildContext context) { return MaterialApp( title: 'Google Authencation Demo', theme: ThemeData( primarySwatch: Colors.blue, ), home: HomePage(), ); } } \u756b\u9762 \u5982\u679c\u7528 Web \u6703\u7121\u6cd5\u505a\u51fa\u6548\u679c\uff0c\u51fa\u73fe\u4ee5\u4e0b\u932f\u8aa4\u8a0a\u606f\uff1a ClientID not set. Either set it on a <meta name=\\\"google-signin-client_id\\\" content=\\\"CLIENT_ID\\\" /> tag, or pass clientId when initializing GoogleSignIn (1) \u53bb\u627e Clitne id : 8073574655-9c9sa7onebk99ju1mf2i5e1kfi5m4hk0.apps.googleusercontent.com (2) \u4fee\u6539\u4ee3\u78bc home.dart \u539f\u672c final GoogleSignIn _googleSignIn = GoogleSignIn(); \u5f8c\u4f86 final GoogleSignIn _googleSignIn = GoogleSignIn(clientId: '8073574655-saga5315djjgbcgsgag5qfmgusl6tqpu.apps.googleusercontent.com'); (3) \u51fa\u73fe\u4ee5\u4e0b\u8a0a\u606f\uff0c\u5247\u76f4\u63a5\u9ede\u9078\uff0c\u7167\u8457\u6b65\u9a5f\u505a\u5373\u53ef\u3002 PlatformException(idpiframe_initialization_failed, Not a valid origin for the client: http://localhost:13529 has not been registered for client ID 8073574655-9c9sa7onebk99ju1mf2i5e1kfi5m4hk0.apps.googleusercontent.com. Please go to https://console.developers.google.com/ and register this origin for your project's client ID., https://developers.google.com/identity/sign-in/web/reference#error_codes, null) \u5df2\u6388\u6b0a\u7684 JavaScript \u4f86\u6e90\uff0c\u8981\u628a\u9019\u500bPort\u52a0\u4e0a\u53bb (4) Flutter run \u6642\uff0c\u6307\u5b9a PORT = 12621 \u53c3\u6578\u3002--web-port=12621 (5) \u518d\u6b21\u6253\u958b\u700f\u89bd\u5668\uff0c\u4e26\u4e14\u6e05\u9664\u5feb\u53d6\u548c\u700f\u89bd\u8cc7\u6599\u3002\u5c31\u53ef\u4ee5\u4e86\u3002","title":"Get Started with Firebase Authentication on Flutter"},{"location":"flutter/#try-out-an-example-app-with-analytics","text":"flutter pub add firebase_analytics flutterfire configure Access the lib directory of the app, then delete the existing main.dart file. \u5c07\u4ee5\u4e0b\u5169\u500b\u6a94\u6848\uff0c\u8907\u88fd\u8cbc\u4e0a\u5230\u5c08\u6848\u4e2d lib/ https://github.com/firebase/flutterfire/blob/master/packages/firebase_analytics/firebase_analytics/example/lib/main.dart https://github.com/firebase/flutterfire/blob/master/packages/firebase_analytics/firebase_analytics/example/lib/tabs_page.dart \u57f7\u884c flutter run \u932f\u8aa4\u8a0a\u606f \u932f\u8aa41 \u932f\u8aa42 \u5982\u679c\u51fa\u73fe Missing google_app_id. Firebase Analytics disabled \u932f\u8aa4\u8a0a\u606f \u4ee3\u8868\u6709\u5169\u500b\u5730\u65b9\u9700\u8981\u4fee\u6539\uff1a - android\\build.gradle ``` classpath 'com.google.gms:google-services:4.3.15' ``` ![Alt text](image-61.png) - android\\app\\build.gradle ``` apply plugin: 'com.google.gms.google-services' ``` ![Alt text](image-62.png) \u6210\u679c","title":"Try out an example app with Analytics"},{"location":"microk8s/","text":"Microk8s \u7b46\u8a18 1.\u5efa\u4e3b\u7bc0\u9ede master (Oracle VM VirtualBox) \u4e0b\u8f09 iso \u5370\u8c61\u6a94\uff0c\u9078\u64c7 ubuntu-22.04.2-desktop-amd64.iso \u958b\u555f Oracle VM VirtualBox \u7ba1\u7406\u54e1\uff0c\u9ede\u9078\u65b0\u589e -> ISO\u6620\u50cf\u9078\u64c7 ubuntu-22.04.2-desktop-amd64.iso -> \u4e0b\u4e00\u6b65 \u3002 \u5e33\u865f\u5bc6\u78bc\u7528\u9810\u8a2d\u5373\u53ef\u3002\u4f7f\u7528\u8005\u540d\u7a31\u70ba vboxuser\uff1b\u5bc6\u78bc\u70ba changeme \u786c\u9ad4\u898f\u683c\uff1a\u9810\u8a2d\u5373\u53ef\u3002 \u865b\u64ec\u786c\u9ad4\uff1a\u9810\u8a2d\u5373\u53ef\u3002 \u6458\u8981\uff1a\u76f4\u63a5\u6309\u5b8c\u6210\u3002 \u986f\u793a master\uff0c\u6703\u5b89\u88dd\u4e00\u6bb5\u6642\u9593\uff0c\u7d04 20 \u5206\u9418\u3002 \u900f\u904e CTRL+ALT+F5 \u5207\u63db\u6210 Terminal \u6a21\u5f0f\uff1bCTRL+ALT+F1 \u5207\u56de UI \u6a21\u5f0f\u3002 \u8f38\u5165 CTRL+ALT+F5 \u8f38\u5165\u5e33\u865f\u3001\u5bc6\u78bc\u5f8c\u767b\u5165 \u53ef\u8f38\u5165 lsb_release -a \u67e5\u770b Ubuntu \u7248\u672c\uff0c\u6703\u986f\u793a Ubuntu 22.04.2 LTS\u3002 \u8a2d\u5b9a\u56fa\u5b9aIP\uff0c\u4e26\u4e14\u8981\u80fd\u5920\u9023\u81f3\u5916\u7db2\u3002 \u5148\u95dc\u9589VM\uff0c\u4e26\u4e14\u5728\u7db2\u8def\u8a2d\u5b9a\uff0c\u586b\u9078\u4ecb\u97622\uff0c\u9078\u64c7\u6a4b\u63a5\u4ecb\u9762\u5361\u3002 \u8acb\u5148\u5efa\u7acb LANBridge \u865b\u64ec\u4ea4\u63db\u5668 \u67e5\u8a62\u76ee\u524dIP hostname -I \u5207\u63db\u76ee\u9304\uff0c\u4e26\u5217\u51fa\u6a94\u6848\uff0c\u5176\u4e2d\u7684 01-network-manager-all.yaml \u70ba\u8981\u4fee\u6539\u7684\u8a2d\u5b9a\u6a94\u3002 cd /etc/netplan/ ls \u5207\u56de su root \uff0c\u5bc6\u78bc\u70ba changeme \u4fee\u6539\u8a2d\u5b9a\u6a94\uff0c\u4fee\u6539\u5b8c\u5f8c\uff0c\u6309\u4e0b CTRL+X\uff0c\u518d\u8f38\u5165 Y\uff0c\u518d\u6309\u4e0b Enter \u5373\u53ef\u3002 sudo nano 01-network-manager-all.yaml \u539f\u672c\uff1a network: version: 2 renderer: NetworkManager \u66f4\u65b0\uff1a #network: # version: 2 # renderer: NetworkManager network: ethernets: enp0s3: dhcp4: true enp0s8: dhcp4: false addresses: [192.168.0.17/24] version: 2 \u56e0\u70ba \u4e59\u592a\u7db2\u8def \u7684 IP \u70ba 192.168.0.14\uff0c\u6240\u4ee5\u5728\u5206\u914d\u6642\uff0c\u53ef\u4ee5\u7db2\u5f8c\u65b0\u589e\u5373\u53ef\u3002 \u57f7\u884c\u66f4\u65b0\uff0c\u8f38\u5165\u6307\u4ee4\uff0c\u4e26\u76f4\u63a5\u6309\u4e0b\uff0c\u5373\u6703\u7acb\u5373\u66f4\u65b0\u3002(\u6216\u91cd\u65b0\u555f\u52d5VM) sudo netplan try \u67e5\u8a62IP\uff0c\u5c31\u6703\u770b\u5230 192.168.0.17 hostname -I \u958b\u555f SSH \u670d\u52d9 \u5148\u5207\u56de su root\uff0c\u4e26\u5b89\u88dd\u8207\u555f\u52d5 SSH apt-get update apt-get install openssh-server sudo service ssh --full-restart \u767b\u5165SSH window cmd -> ssh vboxuser@192.168.0.17 \u6e2c\u8a66\u770b\u770b\u662f\u5426\u53ef\u4ee5\u9023\u7d50\u5916\u7db2 ping google.com \u5982\u4f55\u4fee\u6539 ssh \u7684\u9810\u8a2d port 1. \u767b\u5165 ssh console \u5f8c\uff0c\u57f7\u884c\u4e0b\u5217\u6307\u4ee4: nano /etc/ssh/sshd_config 2 \u5c07: #Port 22 \u6539\u70ba: Port 3333 3 \u5132\u5b58\u9000\u51fa\u5f8c\uff0c console\u5e95\u4e0b\u5728\u57f7\u884cssh \u91cd\u958b\u6307\u4ee4: service ssh restart \u5982\u4f55\u8b93 root \u4e5f\u80fd\u767b\u5165 SSH nano /etc/ssh/sshd_config \u52a0\u5165 PermitRootLogin yes service ssh restart 2.\u5efa\u7bc0\u9ede node (Oracle VM VirtualBox) \u6b65\u9a5f\u540c\u4e0a\uff0cIP \u6539\u70ba 192.168.0.18 \u5373\u53ef\u3002 3.\u5728 master \u8207 node \u4e2d\uff0c\u5b89\u88dd microk8s\u3002(\u5176\u4e2dmetallb\u7684IP\u70ba master\u7684 IP) master node sudo snap install microk8s --classic master microk8s enable metallb:192.168.0.17-192.168.0.100 master node microk8s enable dashboard dns registry istio helm3 storage master microk8s status --wait-ready master microk8s dashboard-proxy metallb\uff1a\u8981\u597d\u5e7e\u5206\u9418\uff0c\u76f4\u5230\u770b\u5230 MetaLB is enabled\u3002 microk8s status --wait-ready\uff0c\u770b\u5230 is runnung \u4ee3\u8868\u6210\u529f\u3002 microk8s dashboard-proxy \u6253\u958b\u700f\u89bd\u5668\uff0c\u7db2\u5740\u70ba\uff1ahttps://192.168.0.17:10443\u3002\u9ede\u9078\u9032\u968e\uff0c\u7e7c\u7e8c\u524d\u5f80\u3002\u8f38\u5165 Token\uff0c\u9ede\u9078 Sign in\u3002 \u5982\u679c \u4e0d\u559c\u6b61\u5148\u5207\u63db su root\uff0c\u6216\u662f\u51fa\u73fe vboxuser is not in the sudoers file. This incident will be reported. Step1. su root Step2. nano /etc/sudoers Step3. \u52a0\u5165 vboxuser ALL=(ALL:ALL) ALL 4. \u5efa\u7acb\u53e2\u96c6\u95dc\u4fc2 \u5728 master \u4e2d\u8f38\u5165 microk8s add-node # \u5982\u679c\u6709\u932f\uff0c\u4ee3\u8868\u6c92\u6709\u6b0a\u9650\uff0c\u8981\u5148\u57f7\u884c\u4ee5\u4e0b\u5169\u884c su root sudo usermod -a -G microk8s vboxuser sudo chown -R vboxuser ~/.kube microk8s add-node \u9078\u64c7 \u975c\u614b\u7684IP microk8s join 192.168.0.17:25000/d3594715ba0d9c6b63b359c7166421de/bc19f9886c4b \u5728 node \u4e2d\u8f38\u5165 microk8s join 192.168.0.17:25000/d3594715ba0d9c6b63b359c7166421de/bc19f9886c4b \u932f\u8aa4\uff1a Contacting cluster at 192.168.0.101 Connection failed. The hostname (node1) of the joining node does not resolve to the IP \"192.168.0.18\". Refusing join (400). \u89e3\u6c7a\uff1a 192.168.0.18 \u70ba node1 \u7684\u56fa\u5b9a IP\uff0c\u8acb\u5728 master\u7684 /etc/hosts \u6a94\u6848\u4e2d\u52a0\u5165 192.168.0.18 node.mshome.net \u4e00\u884c\u3002 nano /etc/hosts add 192.168.0.18 node.mshome.net 192.168.0.18 node \u932f\u8aa4\uff1a Contacting cluster at 192.168.0.17 Connection failed. Invalid token (500). \u89e3\u6c7a\uff1a \u53bb master \u91cd\u65b0\u7522\u751f\u65b0\u7684 token\u3002(microk8s add-node) \u770b\u5230 Waiting for this node to finish joining the cluster. .. .. .. \u5c31\u4ee3\u8868\u6210\u529f\u4e86 5. \u6e2c\u8a66\u8a2d\u5b9a\u6a94 Deployment\uff1a apiVersion: apps/v1 kind: Deployment metadata: name: docker-k8s-demo-deployment labels: app: docker-k8s-demo spec: replicas: 2 selector: matchLabels: app: docker-k8s-demo template: metadata: labels: app: docker-k8s-demo spec: containers: - name: docker-k8s-demo /assets/image: aweit/docker-demo env: - name: \"PORT\" value: \"8090\" \u5982\u679c\u51fa\u73feError: Err/assets/imagePull\uff0c\u4ee3\u8868 Docker Hub \u4e2d\u7684\u9019\u500b /assets/image \u662f Private \u7684\u3002 \u89e3\u6c7a\u8fa6\u6cd5\uff1a \u89e3\u6c7a\u65b9\u6848\u4e00\uff1a\u8abf\u6574 containerd \u7684 Docker Hub Registry \u8a2d\u5b9a [plugins.\"io.containerd.grpc.v1.cri\".registry.configs.\"registry-1.docker.io\".auth] username = \"aweit\" password = \"dckr_pat_x2YLeiKrJ94HdPOosyteGOXLk04\" \u6b63\u78ba\u756b\u9762\u5982\u4e0b\uff1a Service\uff1a apiVersion: v1 kind: Service metadata: name: docker-k8s-demo-service spec: type: LoadBalancer selector: app: docker-k8s-demo ports: - protocol: TCP port: 60000 targetPort: 8090 \u6e2c\u8a66\u9023\u7d50 \u9ede\u9078 Service -> Services \u6253\u958b\u700f\u89bd\u5668\uff0c\u8f38\u5165 http://192.168.0.17:60000/ \u770b Log","title":"Microk8s"},{"location":"microk8s/#microk8s","text":"","title":"Microk8s \u7b46\u8a18"},{"location":"microk8s/#1-master-oracle-vm-virtualbox","text":"\u4e0b\u8f09 iso \u5370\u8c61\u6a94\uff0c\u9078\u64c7 ubuntu-22.04.2-desktop-amd64.iso \u958b\u555f Oracle VM VirtualBox \u7ba1\u7406\u54e1\uff0c\u9ede\u9078\u65b0\u589e -> ISO\u6620\u50cf\u9078\u64c7 ubuntu-22.04.2-desktop-amd64.iso -> \u4e0b\u4e00\u6b65 \u3002 \u5e33\u865f\u5bc6\u78bc\u7528\u9810\u8a2d\u5373\u53ef\u3002\u4f7f\u7528\u8005\u540d\u7a31\u70ba vboxuser\uff1b\u5bc6\u78bc\u70ba changeme \u786c\u9ad4\u898f\u683c\uff1a\u9810\u8a2d\u5373\u53ef\u3002 \u865b\u64ec\u786c\u9ad4\uff1a\u9810\u8a2d\u5373\u53ef\u3002 \u6458\u8981\uff1a\u76f4\u63a5\u6309\u5b8c\u6210\u3002 \u986f\u793a master\uff0c\u6703\u5b89\u88dd\u4e00\u6bb5\u6642\u9593\uff0c\u7d04 20 \u5206\u9418\u3002 \u900f\u904e CTRL+ALT+F5 \u5207\u63db\u6210 Terminal \u6a21\u5f0f\uff1bCTRL+ALT+F1 \u5207\u56de UI \u6a21\u5f0f\u3002 \u8f38\u5165 CTRL+ALT+F5 \u8f38\u5165\u5e33\u865f\u3001\u5bc6\u78bc\u5f8c\u767b\u5165 \u53ef\u8f38\u5165 lsb_release -a \u67e5\u770b Ubuntu \u7248\u672c\uff0c\u6703\u986f\u793a Ubuntu 22.04.2 LTS\u3002 \u8a2d\u5b9a\u56fa\u5b9aIP\uff0c\u4e26\u4e14\u8981\u80fd\u5920\u9023\u81f3\u5916\u7db2\u3002 \u5148\u95dc\u9589VM\uff0c\u4e26\u4e14\u5728\u7db2\u8def\u8a2d\u5b9a\uff0c\u586b\u9078\u4ecb\u97622\uff0c\u9078\u64c7\u6a4b\u63a5\u4ecb\u9762\u5361\u3002 \u8acb\u5148\u5efa\u7acb LANBridge \u865b\u64ec\u4ea4\u63db\u5668 \u67e5\u8a62\u76ee\u524dIP hostname -I \u5207\u63db\u76ee\u9304\uff0c\u4e26\u5217\u51fa\u6a94\u6848\uff0c\u5176\u4e2d\u7684 01-network-manager-all.yaml \u70ba\u8981\u4fee\u6539\u7684\u8a2d\u5b9a\u6a94\u3002 cd /etc/netplan/ ls \u5207\u56de su root \uff0c\u5bc6\u78bc\u70ba changeme \u4fee\u6539\u8a2d\u5b9a\u6a94\uff0c\u4fee\u6539\u5b8c\u5f8c\uff0c\u6309\u4e0b CTRL+X\uff0c\u518d\u8f38\u5165 Y\uff0c\u518d\u6309\u4e0b Enter \u5373\u53ef\u3002 sudo nano 01-network-manager-all.yaml \u539f\u672c\uff1a network: version: 2 renderer: NetworkManager \u66f4\u65b0\uff1a #network: # version: 2 # renderer: NetworkManager network: ethernets: enp0s3: dhcp4: true enp0s8: dhcp4: false addresses: [192.168.0.17/24] version: 2 \u56e0\u70ba \u4e59\u592a\u7db2\u8def \u7684 IP \u70ba 192.168.0.14\uff0c\u6240\u4ee5\u5728\u5206\u914d\u6642\uff0c\u53ef\u4ee5\u7db2\u5f8c\u65b0\u589e\u5373\u53ef\u3002 \u57f7\u884c\u66f4\u65b0\uff0c\u8f38\u5165\u6307\u4ee4\uff0c\u4e26\u76f4\u63a5\u6309\u4e0b\uff0c\u5373\u6703\u7acb\u5373\u66f4\u65b0\u3002(\u6216\u91cd\u65b0\u555f\u52d5VM) sudo netplan try \u67e5\u8a62IP\uff0c\u5c31\u6703\u770b\u5230 192.168.0.17 hostname -I \u958b\u555f SSH \u670d\u52d9 \u5148\u5207\u56de su root\uff0c\u4e26\u5b89\u88dd\u8207\u555f\u52d5 SSH apt-get update apt-get install openssh-server sudo service ssh --full-restart \u767b\u5165SSH window cmd -> ssh vboxuser@192.168.0.17 \u6e2c\u8a66\u770b\u770b\u662f\u5426\u53ef\u4ee5\u9023\u7d50\u5916\u7db2 ping google.com \u5982\u4f55\u4fee\u6539 ssh \u7684\u9810\u8a2d port 1. \u767b\u5165 ssh console \u5f8c\uff0c\u57f7\u884c\u4e0b\u5217\u6307\u4ee4: nano /etc/ssh/sshd_config 2 \u5c07: #Port 22 \u6539\u70ba: Port 3333 3 \u5132\u5b58\u9000\u51fa\u5f8c\uff0c console\u5e95\u4e0b\u5728\u57f7\u884cssh \u91cd\u958b\u6307\u4ee4: service ssh restart \u5982\u4f55\u8b93 root \u4e5f\u80fd\u767b\u5165 SSH nano /etc/ssh/sshd_config \u52a0\u5165 PermitRootLogin yes service ssh restart","title":"1.\u5efa\u4e3b\u7bc0\u9ede master (Oracle VM VirtualBox)"},{"location":"microk8s/#2-node-oracle-vm-virtualbox","text":"\u6b65\u9a5f\u540c\u4e0a\uff0cIP \u6539\u70ba 192.168.0.18 \u5373\u53ef\u3002","title":"2.\u5efa\u7bc0\u9ede node (Oracle VM VirtualBox)"},{"location":"microk8s/#3-master-node-microk8smetallbip-master-ip","text":"master node sudo snap install microk8s --classic master microk8s enable metallb:192.168.0.17-192.168.0.100 master node microk8s enable dashboard dns registry istio helm3 storage master microk8s status --wait-ready master microk8s dashboard-proxy metallb\uff1a\u8981\u597d\u5e7e\u5206\u9418\uff0c\u76f4\u5230\u770b\u5230 MetaLB is enabled\u3002 microk8s status --wait-ready\uff0c\u770b\u5230 is runnung \u4ee3\u8868\u6210\u529f\u3002 microk8s dashboard-proxy \u6253\u958b\u700f\u89bd\u5668\uff0c\u7db2\u5740\u70ba\uff1ahttps://192.168.0.17:10443\u3002\u9ede\u9078\u9032\u968e\uff0c\u7e7c\u7e8c\u524d\u5f80\u3002\u8f38\u5165 Token\uff0c\u9ede\u9078 Sign in\u3002 \u5982\u679c \u4e0d\u559c\u6b61\u5148\u5207\u63db su root\uff0c\u6216\u662f\u51fa\u73fe vboxuser is not in the sudoers file. This incident will be reported. Step1. su root Step2. nano /etc/sudoers Step3. \u52a0\u5165 vboxuser ALL=(ALL:ALL) ALL","title":"3.\u5728 master \u8207 node \u4e2d\uff0c\u5b89\u88dd microk8s\u3002(\u5176\u4e2dmetallb\u7684IP\u70ba master\u7684 IP)"},{"location":"microk8s/#4","text":"\u5728 master \u4e2d\u8f38\u5165 microk8s add-node # \u5982\u679c\u6709\u932f\uff0c\u4ee3\u8868\u6c92\u6709\u6b0a\u9650\uff0c\u8981\u5148\u57f7\u884c\u4ee5\u4e0b\u5169\u884c su root sudo usermod -a -G microk8s vboxuser sudo chown -R vboxuser ~/.kube microk8s add-node \u9078\u64c7 \u975c\u614b\u7684IP microk8s join 192.168.0.17:25000/d3594715ba0d9c6b63b359c7166421de/bc19f9886c4b \u5728 node \u4e2d\u8f38\u5165 microk8s join 192.168.0.17:25000/d3594715ba0d9c6b63b359c7166421de/bc19f9886c4b \u932f\u8aa4\uff1a Contacting cluster at 192.168.0.101 Connection failed. The hostname (node1) of the joining node does not resolve to the IP \"192.168.0.18\". Refusing join (400). \u89e3\u6c7a\uff1a 192.168.0.18 \u70ba node1 \u7684\u56fa\u5b9a IP\uff0c\u8acb\u5728 master\u7684 /etc/hosts \u6a94\u6848\u4e2d\u52a0\u5165 192.168.0.18 node.mshome.net \u4e00\u884c\u3002 nano /etc/hosts add 192.168.0.18 node.mshome.net 192.168.0.18 node \u932f\u8aa4\uff1a Contacting cluster at 192.168.0.17 Connection failed. Invalid token (500). \u89e3\u6c7a\uff1a \u53bb master \u91cd\u65b0\u7522\u751f\u65b0\u7684 token\u3002(microk8s add-node) \u770b\u5230 Waiting for this node to finish joining the cluster. .. .. .. \u5c31\u4ee3\u8868\u6210\u529f\u4e86","title":"4. \u5efa\u7acb\u53e2\u96c6\u95dc\u4fc2"},{"location":"microk8s/#5","text":"Deployment\uff1a apiVersion: apps/v1 kind: Deployment metadata: name: docker-k8s-demo-deployment labels: app: docker-k8s-demo spec: replicas: 2 selector: matchLabels: app: docker-k8s-demo template: metadata: labels: app: docker-k8s-demo spec: containers: - name: docker-k8s-demo /assets/image: aweit/docker-demo env: - name: \"PORT\" value: \"8090\" \u5982\u679c\u51fa\u73feError: Err/assets/imagePull\uff0c\u4ee3\u8868 Docker Hub \u4e2d\u7684\u9019\u500b /assets/image \u662f Private \u7684\u3002 \u89e3\u6c7a\u8fa6\u6cd5\uff1a \u89e3\u6c7a\u65b9\u6848\u4e00\uff1a\u8abf\u6574 containerd \u7684 Docker Hub Registry \u8a2d\u5b9a [plugins.\"io.containerd.grpc.v1.cri\".registry.configs.\"registry-1.docker.io\".auth] username = \"aweit\" password = \"dckr_pat_x2YLeiKrJ94HdPOosyteGOXLk04\" \u6b63\u78ba\u756b\u9762\u5982\u4e0b\uff1a Service\uff1a apiVersion: v1 kind: Service metadata: name: docker-k8s-demo-service spec: type: LoadBalancer selector: app: docker-k8s-demo ports: - protocol: TCP port: 60000 targetPort: 8090 \u6e2c\u8a66\u9023\u7d50 \u9ede\u9078 Service -> Services \u6253\u958b\u700f\u89bd\u5668\uff0c\u8f38\u5165 http://192.168.0.17:60000/ \u770b Log","title":"5. \u6e2c\u8a66\u8a2d\u5b9a\u6a94"},{"location":"nfs/","text":"Use NFS for Persistent Volumes https://microk8s.io/docs/nfs 1. Setup an NFS server \u5047\u8a2d\u5728 192.168.0.16 \u67b6\u8a2d NFS server sudo apt update sudo apt-get install nfs-kernel-server Create a directory to be used for NFS: sudo mkdir -p /srv/nfs sudo chown nobody:nogroup /srv/nfs sudo chmod 0777 /srv/nfs Edit the /etc/exports file. Make sure that the IP addresses of all your MicroK8s nodes are able to mount this share. For example, to allow all IP addresses in the 10.0.0.0/24 subnet sudo mv /etc/exports /etc/exports.bak echo '/srv/nfs 192.168.0.0/24(rw,sync,no_subtree_check,no_root_squash)' | sudo tee /etc/exports \u91cd\u555f sudo systemctl restart nfs-kernel-server.service \u78ba\u8a8d NFS Server\u662f\u5426\u6b63\u5e38\u555f\u52d5 sudo systemctl status nfs-kernel-server.service \u6216\u662f\u53ef\u4ee5\u4f7f\u7528 showmount \u547d\u4ee4\u4f86\u6aa2\u67e5 NFS Server \u958b\u51fa\u4f86\u7684\u76ee\u9304 showmount -e 192.168.0.16 1-1. NFS Client (\u53ef\u9078\uff0c\u4f5c\u70ba\u6e2c\u8a66\u7528) \u4f7f\u7528 apt \u547d\u4ee4\u9032\u884c\u5b89\u88dd NFS Client sudo apt update sudo apt install nfs-common \u4f7f\u7528 showmount \u547d\u4ee4\u6aa2\u67e5 NFS Server \u53ef\u9023\u7dda\u7684\u76ee\u9304 showmount -e 192.168.0.16 ---output--- Export list for 192.168.0.16: /srv/nfs 192.168.0.0/24 \u5982\u679c\u51fa\u73fe\u5835\u585e\u73fe\u8c61(Stuck)\uff0c\u53ef\u80fd\u539f\u56e0\u70ba NFS Server \u7684\u9632\u706b\u7246\u5c0e\u81f4\uff0c\u53ea\u8981\u95dc\u6389\u9632\u706b\u7246\u5373\u53ef( sudo ufw disable )\u3002 \u5efa\u7acb\u639b\u8f09\u7528\u76ee\u9304\uff0c\u4e26\u5c07 NFS Server \u7684\u76ee\u9304\u639b\u8f09\u81f3\u672c\u6a5f\u76ee\u9304 sudo mkdir /srv/nfs sudo mount 192.168.0.16:/srv/nfs /srv/nfs \u5982\u679c\u51fa\u73fe mount.nfs: access denied by server while mounting 192.168.0.16:/srv/nfs \u554f\u984c\uff0c\u53ef\u4ee5\u5148\u900f\u904e sudo mount -t nfs -vvvv 192.168.0.16:/srv/nfs /srv/nfs \u67e5\u770b\u8a73\u7d30\u8cc7\u8a0a\u3002 \u6e2c\u8a66\uff1a \u767b\u5165 ssh vboxuser@192.168.17 # NFS Client \u767b\u5165 ssh vboxuser@192.168.16 # NFS Server \u56e0\u70ba NFS Client \u5c07\u672c\u6a5f\u7684 /srv/nfs \u8cc7\u6599\u593e\uff0c\u7d81\u5b9a\u5728 NFS Server \u7684 /srv/nfs \u8cc7\u6599\u593e \u8cc7\u6599\u593e\u4e2d\uff0c\u6240\u4ee5\u5169\u8005\u8cc7\u6599\u593e\u6703\u540c\u6b65\u3002 \u5148\u5728 NFS Client \u57f7\u884c\u4ee5\u4e0b\u547d\u4ee4 touch /srv/nfs/123.txt ls \u518d\u53bb NFS Server \u57f7\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c31\u6703\u770b\u5230 123.txt \u6a94\u6848\u4e86 cd /srv/nfs ls 2.Install the CSI driver for NFS 3.Create a StorageClass for NFS & Create a new PVC # sc-nfs.yaml --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-csi provisioner: nfs.csi.k8s.io parameters: server: 192.168.0.16 share: /srv/nfs reclaimPolicy: Delete volumeBindingMode: Immediate mountOptions: - hard - nfsvers=4.1 microk8s kubectl apply -f - < sc-nfs.yaml # pvc-nfs.yaml --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc spec: storageClassName: nfs-csi accessModes: [ReadWriteOnce] resources: requests: storage: 5Gi microk8s kubectl apply -f - < pvc-nfs.yaml microk8s kubectl describe pvc my-pvc 4. \u6e2c\u8a66 Pod apiVersion: v1 kind: Pod metadata: name: hwchiu labels: app: hwchiu spec: containers: - name: busybox image: hwchiu/netutils:latest volumeMounts: - name: nfs-volume mountPath: /nfs volumes: - name: nfs-volume persistentVolumeClaim: claimName: my-pvc \u9032\u5165Pod kubectl exec -it hwchiu -n nfs-demo -- /bin/bash ls # \u770b\u770b\u6709\u6c92\u6709\u525b\u525b\u5efa\u7acb\u7684 456.txt \u4ee5\u4e0b\u662f\u9019\u500b Pod \u7684 Volume \u5b8c\u6574\u8cc7\u8a0a pvc \u8cc7\u8a0a pv \u8cc7\u8a0a 5. MySQL + NFS kubectl create namespace kube-demo kubectl create secret generic mysql-pass --from-literal=password=password321 -n kube-demo PV\u3001PVC apiVersion: v1 kind: PersistentVolume metadata: name: mysql-pv labels: pv: mysql-pv spec: storageClassName: nfs-csi capacity: storage: 15Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Recycle nfs: path: /srv/nfs/mysql-pv server: 192.168.0.16 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pvc spec: storageClassName: \"nfs-csi\" accessModes: - ReadWriteMany resources: requests: storage: 10Gi selector: matchLabels: pv: mysql-pv Deployment apiVersion: v1 kind: Service metadata: name: mysql spec: selector: app: mysql ports: - protocol: TCP port: 3306 targetPort: 3306 nodePort: 31306 type: NodePort --- apiVersion: apps/v1 kind: Deployment metadata: name: mysql labels: app: mysql-test spec: replicas: 1 selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: nodeName: node containers: - name: mysql-test image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: password ports: - containerPort: 3306 volumeMounts: - mountPath: \"/var/lib/mysql\" name: mysql-data volumes: - name: mysql-data persistentVolumeClaim: claimName: mysql-pvc P.S\uff1a \u5982\u679c\u4e00\u76f4 Pending \uff0c\u751a\u81f3\u51fa\u73fe CrashLoopBackOff \u7684\u8a0a\u606f\uff0c\u53ef\u4ee5\u57f7\u884c kubectl logs mysql-848b44f4c5-kgdzc -n kube-demo \u770b\u770b\u54ea\u88e1\u51fa\u932f\u3002 \u53ef\u53c3\u8003 https://blog.csdn.net/m0_46090675/article/details/122276216 \u89e3\u6c7a\u6b64\u554f\u984c\u3002 \u6700\u5f8c\u5c31\u6703\u6210\u529f\u4e86~~~ \u9996\u5148\u6aa2\u67e5 NFS Server \u7684\u6a94\u6848\u4e2d\u662f\u5426\u6709 mysql \u7684\u76f8\u95dc\u8cc7\u6599 \u9032\u5165 Pod \u7684\u547d\u4ee4\u5217\u4e2d\uff0c\u67e5\u770b Mysql \u76f8\u95dc\u8cc7\u8a0a kubectl exec -it mysql-5b46fb64b4-t2m4c -n kube-demo -- /bin/bash","title":"NFS"},{"location":"nfs/#use-nfs-for-persistent-volumes","text":"https://microk8s.io/docs/nfs","title":"Use NFS for Persistent Volumes"},{"location":"nfs/#1-setup-an-nfs-server","text":"\u5047\u8a2d\u5728 192.168.0.16 \u67b6\u8a2d NFS server sudo apt update sudo apt-get install nfs-kernel-server Create a directory to be used for NFS: sudo mkdir -p /srv/nfs sudo chown nobody:nogroup /srv/nfs sudo chmod 0777 /srv/nfs Edit the /etc/exports file. Make sure that the IP addresses of all your MicroK8s nodes are able to mount this share. For example, to allow all IP addresses in the 10.0.0.0/24 subnet sudo mv /etc/exports /etc/exports.bak echo '/srv/nfs 192.168.0.0/24(rw,sync,no_subtree_check,no_root_squash)' | sudo tee /etc/exports \u91cd\u555f sudo systemctl restart nfs-kernel-server.service \u78ba\u8a8d NFS Server\u662f\u5426\u6b63\u5e38\u555f\u52d5 sudo systemctl status nfs-kernel-server.service \u6216\u662f\u53ef\u4ee5\u4f7f\u7528 showmount \u547d\u4ee4\u4f86\u6aa2\u67e5 NFS Server \u958b\u51fa\u4f86\u7684\u76ee\u9304 showmount -e 192.168.0.16","title":"1. Setup an NFS server"},{"location":"nfs/#1-1-nfs-client","text":"\u4f7f\u7528 apt \u547d\u4ee4\u9032\u884c\u5b89\u88dd NFS Client sudo apt update sudo apt install nfs-common \u4f7f\u7528 showmount \u547d\u4ee4\u6aa2\u67e5 NFS Server \u53ef\u9023\u7dda\u7684\u76ee\u9304 showmount -e 192.168.0.16 ---output--- Export list for 192.168.0.16: /srv/nfs 192.168.0.0/24 \u5982\u679c\u51fa\u73fe\u5835\u585e\u73fe\u8c61(Stuck)\uff0c\u53ef\u80fd\u539f\u56e0\u70ba NFS Server \u7684\u9632\u706b\u7246\u5c0e\u81f4\uff0c\u53ea\u8981\u95dc\u6389\u9632\u706b\u7246\u5373\u53ef( sudo ufw disable )\u3002 \u5efa\u7acb\u639b\u8f09\u7528\u76ee\u9304\uff0c\u4e26\u5c07 NFS Server \u7684\u76ee\u9304\u639b\u8f09\u81f3\u672c\u6a5f\u76ee\u9304 sudo mkdir /srv/nfs sudo mount 192.168.0.16:/srv/nfs /srv/nfs \u5982\u679c\u51fa\u73fe mount.nfs: access denied by server while mounting 192.168.0.16:/srv/nfs \u554f\u984c\uff0c\u53ef\u4ee5\u5148\u900f\u904e sudo mount -t nfs -vvvv 192.168.0.16:/srv/nfs /srv/nfs \u67e5\u770b\u8a73\u7d30\u8cc7\u8a0a\u3002 \u6e2c\u8a66\uff1a \u767b\u5165 ssh vboxuser@192.168.17 # NFS Client \u767b\u5165 ssh vboxuser@192.168.16 # NFS Server \u56e0\u70ba NFS Client \u5c07\u672c\u6a5f\u7684 /srv/nfs \u8cc7\u6599\u593e\uff0c\u7d81\u5b9a\u5728 NFS Server \u7684 /srv/nfs \u8cc7\u6599\u593e \u8cc7\u6599\u593e\u4e2d\uff0c\u6240\u4ee5\u5169\u8005\u8cc7\u6599\u593e\u6703\u540c\u6b65\u3002 \u5148\u5728 NFS Client \u57f7\u884c\u4ee5\u4e0b\u547d\u4ee4 touch /srv/nfs/123.txt ls \u518d\u53bb NFS Server \u57f7\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c31\u6703\u770b\u5230 123.txt \u6a94\u6848\u4e86 cd /srv/nfs ls","title":"1-1. NFS Client (\u53ef\u9078\uff0c\u4f5c\u70ba\u6e2c\u8a66\u7528)"},{"location":"nfs/#2install-the-csi-driver-for-nfs","text":"","title":"2.Install the CSI driver for NFS"},{"location":"nfs/#3create-a-storageclass-for-nfs-create-a-new-pvc","text":"# sc-nfs.yaml --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-csi provisioner: nfs.csi.k8s.io parameters: server: 192.168.0.16 share: /srv/nfs reclaimPolicy: Delete volumeBindingMode: Immediate mountOptions: - hard - nfsvers=4.1 microk8s kubectl apply -f - < sc-nfs.yaml # pvc-nfs.yaml --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc spec: storageClassName: nfs-csi accessModes: [ReadWriteOnce] resources: requests: storage: 5Gi microk8s kubectl apply -f - < pvc-nfs.yaml microk8s kubectl describe pvc my-pvc","title":"3.Create a StorageClass for NFS &amp; Create a new PVC"},{"location":"nfs/#4-pod","text":"apiVersion: v1 kind: Pod metadata: name: hwchiu labels: app: hwchiu spec: containers: - name: busybox image: hwchiu/netutils:latest volumeMounts: - name: nfs-volume mountPath: /nfs volumes: - name: nfs-volume persistentVolumeClaim: claimName: my-pvc \u9032\u5165Pod kubectl exec -it hwchiu -n nfs-demo -- /bin/bash ls # \u770b\u770b\u6709\u6c92\u6709\u525b\u525b\u5efa\u7acb\u7684 456.txt \u4ee5\u4e0b\u662f\u9019\u500b Pod \u7684 Volume \u5b8c\u6574\u8cc7\u8a0a pvc \u8cc7\u8a0a pv \u8cc7\u8a0a","title":"4. \u6e2c\u8a66 Pod"},{"location":"nfs/#5-mysql-nfs","text":"kubectl create namespace kube-demo kubectl create secret generic mysql-pass --from-literal=password=password321 -n kube-demo PV\u3001PVC apiVersion: v1 kind: PersistentVolume metadata: name: mysql-pv labels: pv: mysql-pv spec: storageClassName: nfs-csi capacity: storage: 15Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Recycle nfs: path: /srv/nfs/mysql-pv server: 192.168.0.16 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pvc spec: storageClassName: \"nfs-csi\" accessModes: - ReadWriteMany resources: requests: storage: 10Gi selector: matchLabels: pv: mysql-pv Deployment apiVersion: v1 kind: Service metadata: name: mysql spec: selector: app: mysql ports: - protocol: TCP port: 3306 targetPort: 3306 nodePort: 31306 type: NodePort --- apiVersion: apps/v1 kind: Deployment metadata: name: mysql labels: app: mysql-test spec: replicas: 1 selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: nodeName: node containers: - name: mysql-test image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: password ports: - containerPort: 3306 volumeMounts: - mountPath: \"/var/lib/mysql\" name: mysql-data volumes: - name: mysql-data persistentVolumeClaim: claimName: mysql-pvc P.S\uff1a \u5982\u679c\u4e00\u76f4 Pending \uff0c\u751a\u81f3\u51fa\u73fe CrashLoopBackOff \u7684\u8a0a\u606f\uff0c\u53ef\u4ee5\u57f7\u884c kubectl logs mysql-848b44f4c5-kgdzc -n kube-demo \u770b\u770b\u54ea\u88e1\u51fa\u932f\u3002 \u53ef\u53c3\u8003 https://blog.csdn.net/m0_46090675/article/details/122276216 \u89e3\u6c7a\u6b64\u554f\u984c\u3002 \u6700\u5f8c\u5c31\u6703\u6210\u529f\u4e86~~~ \u9996\u5148\u6aa2\u67e5 NFS Server \u7684\u6a94\u6848\u4e2d\u662f\u5426\u6709 mysql \u7684\u76f8\u95dc\u8cc7\u6599 \u9032\u5165 Pod \u7684\u547d\u4ee4\u5217\u4e2d\uff0c\u67e5\u770b Mysql \u76f8\u95dc\u8cc7\u8a0a kubectl exec -it mysql-5b46fb64b4-t2m4c -n kube-demo -- /bin/bash","title":"5. MySQL + NFS"},{"location":"practicek8s/","text":"Kubernetes \u88dc\u5145 \u932f\u8aa4 Failed to allocate IP: No available IPs \u56e0\u70ba\u7576\u521d\u5728\u958b\u555f microk8s enable metallb:192.168.0.17-192.168.0.17 \u592a\u5c11\u4e86\u3002\u50cf\u662f\u4f8b\u5b50\u70ba\u53ea\u6709\u958b\u4e00\u500b\u3002 \u6b64\u6642\uff0c\u53ef\u4ee5\u5148\u95dc\u6389 metallb\uff0c\u518d\u91cd\u65b0\u958b\u555f\uff0c\u4e26\u4e14\u5c07IP\u6578\u5b57\u8b8a\u591a\u3002 microk8s disable metallb microk8s enable metallb:192.168.0.17-192.168.0.100 \u53c3\u8003\u7db2\u5740\uff1a https://discuss.kubernetes.io/t/addon-metallb/11790 \u77e5\u8b58 \u900f\u904e MicroK8s \u8a8d\u8b58 Kubernetes \u7684 Service Account (\u670d\u52d9\u5e33\u6236) \u53c3\u8003\u7db2\u5740\uff1a https://blog.miniasp.com/post/2022/08/24/Understanding-Service-Account-in-Kubernetes-through-MicroK8s \u5efa\u7acb\u5225\u540d\uff1a\u8b93 microk8s.kubectl = kubectl sudo snap alias microk8s.kubectl kubectl \u5982\u4f55\u53d6\u5f97 Pod \u7684\u8cc7\u8a0a\uff0c\u4e26\u4ee5 yaml \u5448\u73fe\u3002 kubectl get pod microbot -n dev -o yaml \u9032\u5165Pod\u7684console kubectl exec microbot -it -n dev -- sh RBAC k8s \u5728 1.8 \u7248\u4e4b\u5f8c\uff0c\u5f15\u7528\u4e86 Role-Base Access Control (RBAC\uff0c\u57fa\u65bc\u89d2\u8272\u7684\u8a2a\u554f\u63a7\u5236\uff0c\u597d\u50cf\u6709\u9ede\u7e5e\u820c) \u505a\u70ba\u6388\u6b0a (Authorization) \u7684\u57fa\u790e\uff0c\u4e5f\u5c31\u662f\u4e00\u7a2e\u7ba1\u5236\u8a2a\u554f k8s API \u7684\u6a5f\u5236\u3002\u7ba1\u7406\u8005\u53ef\u4ee5\u900f\u904e rbac.authorization.k8s.io \u9019\u500b API \u7fa4\u7d44\u4f86\u9032\u884c\u52d5\u614b\u7684\u7ba1\u7406\u914d\u7f6e\u3002\u5f15\u7528 https://ithelp.ithome.com.tw/articles/10195944 kubectl create namespace dev kubectl label namespace dev name=dev kubectl run microbot --/assets/image=dontrebootme/microbot:v1 -n dev kubectl create serviceaccount monitor -n dev kubectl create clusterrole aweit --verb=' ' --resource=' ' kubectl create clusterrolebinding aweit --clusterrole=aweit --serviceaccount='dev:monitor' microk8s kubectl create token monitor \u7bc4\u4f8b https://blog.miniasp.com/post/2022/08/24/Understanding-Service-Account-in-Kubernetes-through-MicroK8s Dashboard \u8207 Kubeconfig \u958b\u555f Dashboard microk8s enable dashboard \u767b\u5165\u9801\u9762 \u5982\u4f55\u7522\u751f .kueconfig mircok8s config \u5728\u672c\u6a5f\u5efa\u7acb .kubeconfig \u6a94\u6848\uff0c\u5c07\u4e0a\u5716\u7684\u6587\u5b57\u8cbc\u4e0a\u53bb\u3002 \u91cd\u65b0\u4e0a\u50b3 .kubeconfig \u6a94\u6848\u5373\u53ef\u3002 Token microk8s kubectl create token [service account] microk8s kubectl create token default Volume Type\uff1a emptyDir\u3001hostPath\u3001local\u3001nfs\u3001persistentVolumeClaim apiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - image: registry.k8s.io/test-webserver name: test-container volumeMounts: - mountPath: /test-pd name: test-volume volumes: - name: test-volume hostPath: # \u5bbf\u4e3b\u4e0a\u76ee\u5f55\u4f4d\u7f6e path: /data # \u6b64\u5b57\u6bb5\u4e3a\u53ef\u9009 type: DirectoryOrCreate nfs [Ubuntu NFS \u5b89\u88dd\u6559\u5b78] https://blog.devcloud.com.tw/ubuntu-nfs-install/ \u4f7f\u7528\u8aaa\u660e https://www.hwchiu.com/kubernetes-storage-ii.html NFS Server IP\uff1a 192.168.0.17 showmount -e 192.168.0.17 \u5982\u679c\u8981\u65b0\u589e\u639b\u8f09\u76ee\u9304 # \u5efa\u7acb\u5171\u4eab\u8cc7\u6599\u593e sudo mkdir /opt/nfsshare # \u7de8\u8f2f NFS Server \u7684 Expose \u8a2d\u5b9a sudo nano /etc/exports ... /opt/nfsshare 192.168.0.0/24(rw,sync,no_subtree_check,no_root_squash) # \u91cd\u555f sudo systemctl restart nfs-kernel-server.service # \u986f\u793a Mount \u8cc7\u8a0a showmount -e 192.168.0.17 PV/PVC/Pod apiVersion: v1 kind: PersistentVolume metadata: name: nfs spec: capacity: storage: 1Mi accessModes: - ReadWriteMany nfs: server: 192.168.0.17 path: \"/opt/nfsshare\" --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs spec: accessModes: - ReadOnlyMany storageClassName: \"\" resources: requests: storage: 1Mi --- apiVersion: v1 kind: Pod metadata: name: hwchiu labels: app: hwchiu spec: containers: - name: busybox image: hwchiu/netutils:latest volumeMounts: - name: nfs-volume mountPath: /nfs volumes: - name: nfs-volume persistentVolumeClaim: claimName: nfs Tomcat & Volume # PV: \u5148\u6709 nfs server\uff0c\u4e26\u4e14\u5efa\u7acb\u597d\u5c0d\u61c9\u7684\u8cc7\u6599\u593e\uff0c\u4f8b\u5982\uff1a/opt/nfsshare/demo/webapps apiVersion: v1 kind: PersistentVolume metadata: name: nfs-demo-tomcat spec: capacity: storage: 1Mi accessModes: - ReadOnlyMany nfs: server: 192.168.0.17 path: \"/opt/nfsshare/demo/webapps\" --- # PVC: \u5efa\u7acb PV \u8acb\u6c42 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs-demo-tomcat spec: accessModes: - ReadOnlyMany storageClassName: \"\" resources: requests: storage: 1Mi --- # Deployment: tomcat \u93e1\u50cf\uff0c\u7d81\u5b9a nfs \u5171\u7528\u8cc7\u6599\u593e\u3002 # \u56e0\u70ba tomcat 8 \u4ee5\u5f8c\uff0c\u6703\u591a\u4e00\u500b webapps.dir \u8cc7\u6599\u593e\uff0c\u6240\u4ee5\u8981\u57f7\u884c cp -r webapps.dist/. webapps/ \u624d\u80fd\u5c07\u8cc7\u6599\u593e\u8207\u6a94\u6848\u5168\u90e8\u642c\u79fb\u904e\u53bb\u3002 apiVersion: apps/v1 kind: Deployment metadata: name: tomcat-deployment-nautilus labels: name: webdep app: demo spec: replicas: 1 selector: matchLabels: name: webpod app: demo template: metadata: name: webpod labels: name: webpod app: demo spec: containers: - name: tomcat-container-nautilus image: tomcat ports: - containerPort: 8080 volumeMounts: - name: nfs-volume mountPath: /usr/local/tomcat/webapps volumes: - name: nfs-volume persistentVolumeClaim: claimName: nfs-demo-tomcat --- # Service apiVersion: v1 kind: Service metadata: name: tomcat-service-nautilus spec: selector: name: webpod app: demo ports: - port: 80 targetPort: 8080 --- # IngressRoute apiVersion: traefik.io/v1alpha1 kind: IngressRoute metadata: name: tomcat-ingress-route namespace: default spec: entryPoints: - websecure routes: - kind: Rule match: Host(`localhost.mic.com.tw`) && PathPrefix(`/tomcat`) middlewares: - name: test-stripprefix - name: test-errors services: - name: tomcat-service-nautilus port: 80 tls: secretName: mic-tls \u628a webapps.dist \u76ee\u9304\u4e0b\u7684\u6240\u6709\u6a94\u6848\uff0c\u8907\u88fd\u5230 webapps \u5e95\u4e0b cp -r webapps.dist/. webapps/ traefik: \u628a /tomcat \u53bb\u6389 # Middleware apiVersion: traefik.io/v1alpha1 kind: Middleware metadata: name: test-stripprefix spec: stripPrefix: prefixes: - /tomcat https://localhost.mic.com.tw/tomcat/ Namespace \u72c0\u614b\u70ba Terminal \u6642\uff0c\u5982\u4f55\u522a\u6389\uff1f kubectl delete namespace longhorn-system kubectl get ns/longhorn-system -o json > longhorn-system.json nano longhorn-system.json -> \u6e05\u9664 \"spec\": { \"finalizers\": [ \"kubernetes\" ] } -> \"spec\": { \"finalizers\": [] } kubectl replace --raw \"/api/v1/namespaces/longhorn-system/finalize\" -f ./longhorn-system.json","title":"Kubernetes"},{"location":"practicek8s/#kubernetes","text":"","title":"Kubernetes \u88dc\u5145"},{"location":"practicek8s/#_1","text":"Failed to allocate IP: No available IPs \u56e0\u70ba\u7576\u521d\u5728\u958b\u555f microk8s enable metallb:192.168.0.17-192.168.0.17 \u592a\u5c11\u4e86\u3002\u50cf\u662f\u4f8b\u5b50\u70ba\u53ea\u6709\u958b\u4e00\u500b\u3002 \u6b64\u6642\uff0c\u53ef\u4ee5\u5148\u95dc\u6389 metallb\uff0c\u518d\u91cd\u65b0\u958b\u555f\uff0c\u4e26\u4e14\u5c07IP\u6578\u5b57\u8b8a\u591a\u3002 microk8s disable metallb microk8s enable metallb:192.168.0.17-192.168.0.100 \u53c3\u8003\u7db2\u5740\uff1a https://discuss.kubernetes.io/t/addon-metallb/11790","title":"\u932f\u8aa4"},{"location":"practicek8s/#_2","text":"\u900f\u904e MicroK8s \u8a8d\u8b58 Kubernetes \u7684 Service Account (\u670d\u52d9\u5e33\u6236) \u53c3\u8003\u7db2\u5740\uff1a https://blog.miniasp.com/post/2022/08/24/Understanding-Service-Account-in-Kubernetes-through-MicroK8s \u5efa\u7acb\u5225\u540d\uff1a\u8b93 microk8s.kubectl = kubectl sudo snap alias microk8s.kubectl kubectl \u5982\u4f55\u53d6\u5f97 Pod \u7684\u8cc7\u8a0a\uff0c\u4e26\u4ee5 yaml \u5448\u73fe\u3002 kubectl get pod microbot -n dev -o yaml \u9032\u5165Pod\u7684console kubectl exec microbot -it -n dev -- sh RBAC k8s \u5728 1.8 \u7248\u4e4b\u5f8c\uff0c\u5f15\u7528\u4e86 Role-Base Access Control (RBAC\uff0c\u57fa\u65bc\u89d2\u8272\u7684\u8a2a\u554f\u63a7\u5236\uff0c\u597d\u50cf\u6709\u9ede\u7e5e\u820c) \u505a\u70ba\u6388\u6b0a (Authorization) \u7684\u57fa\u790e\uff0c\u4e5f\u5c31\u662f\u4e00\u7a2e\u7ba1\u5236\u8a2a\u554f k8s API \u7684\u6a5f\u5236\u3002\u7ba1\u7406\u8005\u53ef\u4ee5\u900f\u904e rbac.authorization.k8s.io \u9019\u500b API \u7fa4\u7d44\u4f86\u9032\u884c\u52d5\u614b\u7684\u7ba1\u7406\u914d\u7f6e\u3002\u5f15\u7528 https://ithelp.ithome.com.tw/articles/10195944 kubectl create namespace dev kubectl label namespace dev name=dev kubectl run microbot --/assets/image=dontrebootme/microbot:v1 -n dev kubectl create serviceaccount monitor -n dev kubectl create clusterrole aweit --verb=' ' --resource=' ' kubectl create clusterrolebinding aweit --clusterrole=aweit --serviceaccount='dev:monitor' microk8s kubectl create token monitor \u7bc4\u4f8b https://blog.miniasp.com/post/2022/08/24/Understanding-Service-Account-in-Kubernetes-through-MicroK8s Dashboard \u8207 Kubeconfig \u958b\u555f Dashboard microk8s enable dashboard \u767b\u5165\u9801\u9762 \u5982\u4f55\u7522\u751f .kueconfig mircok8s config \u5728\u672c\u6a5f\u5efa\u7acb .kubeconfig \u6a94\u6848\uff0c\u5c07\u4e0a\u5716\u7684\u6587\u5b57\u8cbc\u4e0a\u53bb\u3002 \u91cd\u65b0\u4e0a\u50b3 .kubeconfig \u6a94\u6848\u5373\u53ef\u3002 Token microk8s kubectl create token [service account] microk8s kubectl create token default","title":"\u77e5\u8b58"},{"location":"practicek8s/#volume","text":"Type\uff1a emptyDir\u3001hostPath\u3001local\u3001nfs\u3001persistentVolumeClaim apiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - image: registry.k8s.io/test-webserver name: test-container volumeMounts: - mountPath: /test-pd name: test-volume volumes: - name: test-volume hostPath: # \u5bbf\u4e3b\u4e0a\u76ee\u5f55\u4f4d\u7f6e path: /data # \u6b64\u5b57\u6bb5\u4e3a\u53ef\u9009 type: DirectoryOrCreate nfs [Ubuntu NFS \u5b89\u88dd\u6559\u5b78] https://blog.devcloud.com.tw/ubuntu-nfs-install/ \u4f7f\u7528\u8aaa\u660e https://www.hwchiu.com/kubernetes-storage-ii.html NFS Server IP\uff1a 192.168.0.17 showmount -e 192.168.0.17 \u5982\u679c\u8981\u65b0\u589e\u639b\u8f09\u76ee\u9304 # \u5efa\u7acb\u5171\u4eab\u8cc7\u6599\u593e sudo mkdir /opt/nfsshare # \u7de8\u8f2f NFS Server \u7684 Expose \u8a2d\u5b9a sudo nano /etc/exports ... /opt/nfsshare 192.168.0.0/24(rw,sync,no_subtree_check,no_root_squash) # \u91cd\u555f sudo systemctl restart nfs-kernel-server.service # \u986f\u793a Mount \u8cc7\u8a0a showmount -e 192.168.0.17 PV/PVC/Pod apiVersion: v1 kind: PersistentVolume metadata: name: nfs spec: capacity: storage: 1Mi accessModes: - ReadWriteMany nfs: server: 192.168.0.17 path: \"/opt/nfsshare\" --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs spec: accessModes: - ReadOnlyMany storageClassName: \"\" resources: requests: storage: 1Mi --- apiVersion: v1 kind: Pod metadata: name: hwchiu labels: app: hwchiu spec: containers: - name: busybox image: hwchiu/netutils:latest volumeMounts: - name: nfs-volume mountPath: /nfs volumes: - name: nfs-volume persistentVolumeClaim: claimName: nfs","title":"Volume"},{"location":"practicek8s/#tomcat-volume","text":"# PV: \u5148\u6709 nfs server\uff0c\u4e26\u4e14\u5efa\u7acb\u597d\u5c0d\u61c9\u7684\u8cc7\u6599\u593e\uff0c\u4f8b\u5982\uff1a/opt/nfsshare/demo/webapps apiVersion: v1 kind: PersistentVolume metadata: name: nfs-demo-tomcat spec: capacity: storage: 1Mi accessModes: - ReadOnlyMany nfs: server: 192.168.0.17 path: \"/opt/nfsshare/demo/webapps\" --- # PVC: \u5efa\u7acb PV \u8acb\u6c42 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs-demo-tomcat spec: accessModes: - ReadOnlyMany storageClassName: \"\" resources: requests: storage: 1Mi --- # Deployment: tomcat \u93e1\u50cf\uff0c\u7d81\u5b9a nfs \u5171\u7528\u8cc7\u6599\u593e\u3002 # \u56e0\u70ba tomcat 8 \u4ee5\u5f8c\uff0c\u6703\u591a\u4e00\u500b webapps.dir \u8cc7\u6599\u593e\uff0c\u6240\u4ee5\u8981\u57f7\u884c cp -r webapps.dist/. webapps/ \u624d\u80fd\u5c07\u8cc7\u6599\u593e\u8207\u6a94\u6848\u5168\u90e8\u642c\u79fb\u904e\u53bb\u3002 apiVersion: apps/v1 kind: Deployment metadata: name: tomcat-deployment-nautilus labels: name: webdep app: demo spec: replicas: 1 selector: matchLabels: name: webpod app: demo template: metadata: name: webpod labels: name: webpod app: demo spec: containers: - name: tomcat-container-nautilus image: tomcat ports: - containerPort: 8080 volumeMounts: - name: nfs-volume mountPath: /usr/local/tomcat/webapps volumes: - name: nfs-volume persistentVolumeClaim: claimName: nfs-demo-tomcat --- # Service apiVersion: v1 kind: Service metadata: name: tomcat-service-nautilus spec: selector: name: webpod app: demo ports: - port: 80 targetPort: 8080 --- # IngressRoute apiVersion: traefik.io/v1alpha1 kind: IngressRoute metadata: name: tomcat-ingress-route namespace: default spec: entryPoints: - websecure routes: - kind: Rule match: Host(`localhost.mic.com.tw`) && PathPrefix(`/tomcat`) middlewares: - name: test-stripprefix - name: test-errors services: - name: tomcat-service-nautilus port: 80 tls: secretName: mic-tls \u628a webapps.dist \u76ee\u9304\u4e0b\u7684\u6240\u6709\u6a94\u6848\uff0c\u8907\u88fd\u5230 webapps \u5e95\u4e0b cp -r webapps.dist/. webapps/ traefik: \u628a /tomcat \u53bb\u6389 # Middleware apiVersion: traefik.io/v1alpha1 kind: Middleware metadata: name: test-stripprefix spec: stripPrefix: prefixes: - /tomcat https://localhost.mic.com.tw/tomcat/","title":"Tomcat &amp; Volume"},{"location":"practicek8s/#namespace-terminal","text":"kubectl delete namespace longhorn-system kubectl get ns/longhorn-system -o json > longhorn-system.json nano longhorn-system.json -> \u6e05\u9664 \"spec\": { \"finalizers\": [ \"kubernetes\" ] } -> \"spec\": { \"finalizers\": [] } kubectl replace --raw \"/api/v1/namespaces/longhorn-system/finalize\" -f ./longhorn-system.json","title":"Namespace \u72c0\u614b\u70ba Terminal \u6642\uff0c\u5982\u4f55\u522a\u6389\uff1f"},{"location":"sonarqube/","text":"SonarQube \u5b89\u88dd docker run -d --name sonarqube -e SONAR_ES_BOOTSTRAP_CHECKS_DISABLE=true -p 9000:9000 sonarqube:latest \u767b\u5165 http://localhost:9000/ login: admin password: admin \u9700\u8981\u4fee\u6539\u5bc6\u78bc password: mitacadmin \u5169\u500b\u53c3\u6578 sonar_host: http://127.0.0.1:9000/ sonar_token: sqa_2d932888615a4ca75bf80e889a422a9bde711ec3 User > My Account > Security \u8207 Drone \u6574\u5408 \u5efa\u7acb\u5169\u500b secret sonar_host: <http://127.0.0.1:9000/> sonar_token: sqa_2d932888615a4ca75bf80e889a422a9bde711ec3 .drone.yml \u7bc4\u4f8b (\u8981\u653e\u5728 Package \u5f8c) name: code-analysis /assets/image: aosapps/drone-sonar-plugin settings: sonar_host: from_secret: sonar_host sonar_token: from_secret: sonar_token commands: sonar-scanner -Dsonar.projectKey=docker-demo -Dsonar.tests=src/test/java -Dsonar.sources=src/main/java -Dsonar.java.libraries=./target/classes -Dsonar.java.binaries=./target/classes -Dsonar.host.url= http://172.31.93.122:9000/ -Dsonar.login=sqa_2d932888615a4ca75bf80e889a422a9bde711ec3","title":"SonarQube"},{"location":"sonarqube/#sonarqube","text":"\u5b89\u88dd docker run -d --name sonarqube -e SONAR_ES_BOOTSTRAP_CHECKS_DISABLE=true -p 9000:9000 sonarqube:latest \u767b\u5165 http://localhost:9000/ login: admin password: admin \u9700\u8981\u4fee\u6539\u5bc6\u78bc password: mitacadmin \u5169\u500b\u53c3\u6578 sonar_host: http://127.0.0.1:9000/ sonar_token: sqa_2d932888615a4ca75bf80e889a422a9bde711ec3 User > My Account > Security \u8207 Drone \u6574\u5408 \u5efa\u7acb\u5169\u500b secret sonar_host: <http://127.0.0.1:9000/> sonar_token: sqa_2d932888615a4ca75bf80e889a422a9bde711ec3 .drone.yml \u7bc4\u4f8b (\u8981\u653e\u5728 Package \u5f8c) name: code-analysis /assets/image: aosapps/drone-sonar-plugin settings: sonar_host: from_secret: sonar_host sonar_token: from_secret: sonar_token commands: sonar-scanner -Dsonar.projectKey=docker-demo -Dsonar.tests=src/test/java -Dsonar.sources=src/main/java -Dsonar.java.libraries=./target/classes -Dsonar.java.binaries=./target/classes -Dsonar.host.url= http://172.31.93.122:9000/ -Dsonar.login=sqa_2d932888615a4ca75bf80e889a422a9bde711ec3","title":"SonarQube"},{"location":"spring-security/","text":"(\u5be6\u4f5c)JWT Authentication and Authorization with Spring Boot 3 and Spring Security 6 + MySQL \u53c3\u8003\u8cc7\u6599\uff1a Bezkoder: https://www.bezkoder.com/spring-boot-security-jwt/ Truong Bui: https://medium.com/@truongbui95/jwt-authentication-and-authorization-with-spring-boot-3-and-spring-security-6-2f90f9337421 New Project docker-compose.yaml \u8a2d\u5b9a version: '3.1' services: mysql: image: mysql:8.0 command: --default-authentication-plugin=mysql_native_password restart: always environment: MYSQL_ROOT_PASSWORD: password MYSQL_DATABASE: db MYSQL_USER: user MYSQL_PASSWORD: password ports: - \"3306:3306\" volumes: - mysql-db:/var/lib/mysql adminer: image: adminer restart: always ports: - 8080:8080 volumes: mysql-db: docker-compose up -d \u958b\u555f http://localhost:8080/ \u5982\u679c\u4e0d\u559c\u6b61\u9019\u500b\u4ecb\u9762\uff0c\u53ef\u4ee5\u6539\u7528 MySQL Workbench application.yaml \u8a2d\u5b9a \u5148\u628a\u9810\u8a2d\u7684 properties \u6539\u6210 yaml \u6a94\u6848 applicaion.yaml spring: datasource: url: jdbc:mysql://172.31.93.122:3306/db username: user password: password driver-class-name: com.mysql.cj.jdbc.Driver jpa: hibernate: ddl-auto: update show-sql: true properties: hibernate: format_sql: true database-platform: org.hibernate.dialect.MySQL8Dialect # App Properties bezkoder: app: jwtCookieName: bezkoder jwtSecret: ======================BezKoder=Spring=========================== jwtExpirationMs: 86400000 pom.xml \u8a2d\u5b9a \u65b0\u589e JWT \u76f8\u95dc\u5957\u4ef6 <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-validation</artifactId> </dependency> <dependency> <groupId>io.jsonwebtoken</groupId> <artifactId>jjwt-api</artifactId> <version>0.11.5</version> </dependency> <dependency> <groupId>io.jsonwebtoken</groupId> <artifactId>jjwt-impl</artifactId> <version>0.11.5</version> <scope>runtime</scope> </dependency> <dependency> <groupId>io.jsonwebtoken</groupId> <artifactId>jjwt-jackson</artifactId> <version>0.11.5</version> <scope>runtime</scope> </dependency> \u5c08\u6848\u7d50\u69cb \u53c3\u8003\u7a0b\u5f0f DemoService https://github.com/aweit-zhu/DemoService","title":"(\u5be6\u4f5c)JWT Authentication and Authorization with Spring Boot 3 and Spring Security 6"},{"location":"spring-security/#jwt-authentication-and-authorization-with-spring-boot-3-and-spring-security-6-mysql","text":"\u53c3\u8003\u8cc7\u6599\uff1a Bezkoder: https://www.bezkoder.com/spring-boot-security-jwt/ Truong Bui: https://medium.com/@truongbui95/jwt-authentication-and-authorization-with-spring-boot-3-and-spring-security-6-2f90f9337421","title":"(\u5be6\u4f5c)JWT Authentication and Authorization with Spring Boot 3 and Spring Security 6 + MySQL"},{"location":"spring-security/#new-project","text":"","title":"New Project"},{"location":"spring-security/#docker-composeyaml","text":"version: '3.1' services: mysql: image: mysql:8.0 command: --default-authentication-plugin=mysql_native_password restart: always environment: MYSQL_ROOT_PASSWORD: password MYSQL_DATABASE: db MYSQL_USER: user MYSQL_PASSWORD: password ports: - \"3306:3306\" volumes: - mysql-db:/var/lib/mysql adminer: image: adminer restart: always ports: - 8080:8080 volumes: mysql-db: docker-compose up -d \u958b\u555f http://localhost:8080/ \u5982\u679c\u4e0d\u559c\u6b61\u9019\u500b\u4ecb\u9762\uff0c\u53ef\u4ee5\u6539\u7528 MySQL Workbench","title":"docker-compose.yaml \u8a2d\u5b9a"},{"location":"spring-security/#applicationyaml","text":"\u5148\u628a\u9810\u8a2d\u7684 properties \u6539\u6210 yaml \u6a94\u6848 applicaion.yaml spring: datasource: url: jdbc:mysql://172.31.93.122:3306/db username: user password: password driver-class-name: com.mysql.cj.jdbc.Driver jpa: hibernate: ddl-auto: update show-sql: true properties: hibernate: format_sql: true database-platform: org.hibernate.dialect.MySQL8Dialect # App Properties bezkoder: app: jwtCookieName: bezkoder jwtSecret: ======================BezKoder=Spring=========================== jwtExpirationMs: 86400000","title":"application.yaml \u8a2d\u5b9a"},{"location":"spring-security/#pomxml","text":"\u65b0\u589e JWT \u76f8\u95dc\u5957\u4ef6 <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-validation</artifactId> </dependency> <dependency> <groupId>io.jsonwebtoken</groupId> <artifactId>jjwt-api</artifactId> <version>0.11.5</version> </dependency> <dependency> <groupId>io.jsonwebtoken</groupId> <artifactId>jjwt-impl</artifactId> <version>0.11.5</version> <scope>runtime</scope> </dependency> <dependency> <groupId>io.jsonwebtoken</groupId> <artifactId>jjwt-jackson</artifactId> <version>0.11.5</version> <scope>runtime</scope> </dependency>","title":"pom.xml \u8a2d\u5b9a"},{"location":"spring-security/#_1","text":"","title":"\u5c08\u6848\u7d50\u69cb"},{"location":"spring-security/#_2","text":"DemoService https://github.com/aweit-zhu/DemoService","title":"\u53c3\u8003\u7a0b\u5f0f"},{"location":"traefik/","text":"Traefik With K8S + CRD (Custom Resource Definitions) IngressRoute Definition # Install Traefik Resource Definitions: kubectl apply -f https://raw.githubusercontent.com/traefik/traefik/v2.10/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml # Install RBAC for Traefik: kubectl apply -f https://raw.githubusercontent.com/traefik/traefik/v2.10/docs/content/reference/dynamic-configuration/kubernetes-crd-rbac.yml Services kubectl apply -f https://raw.githubusercontent.com/traefik/traefik/v2.10/docs/content/user-guides/crd-acme/02-services.yml \u4fee\u6539 traefik apiVersion: v1 kind: Service metadata: name: traefik spec: type: LoadBalancer # \u591a\u52a0\u9019\u4e00\u884c\uff0c\u66b4\u9732\u8857\u53e3 ports: - protocol: TCP name: web # web \u5165\u53e3 port: 8000 - protocol: TCP name: admin # dashboard \u5165\u53e3 port: 8080 - protocol: TCP name: websecure # web https \u5165\u53e3 port: 4443 selector: app: traefik - Deployments kubectl apply -f https://raw.githubusercontent.com/traefik/traefik/v2.10/docs/content/user-guides/crd-acme/03-deployments.yml Traefik Routers ``` apiVersion: traefik.io/v1alpha1 kind: IngressRoute metadata: name: simpleingressroute namespace: default spec: entryPoints: - web routes: match: Host( 192.168.0.17 ) && PathPrefix( /notls ) kind: Rule services: name: whoami port: 80 ``` apiVersion: traefik.io/v1alpha1 kind: IngressRoute metadata: name: docker-demo namespace: default spec: entryPoints: - web routes: - match: PathPrefix(`/docker-demo`) kind: Rule services: - name: myapp port: 80 Dashboard http://192.168.0.23:8080/dashboard/#/ \u6e2c\u8a66 URL\uff1a http://192.168.0.23:8000/notls Middlewares \u6982\u5ff5 \u4fee\u6539 IngressRoute apiVersion: traefik.io/v1alpha1 kind: IngressRoute metadata: name: simpleingressroute namespace: default spec: entryPoints: - web routes: - match: Host(`192.168.0.23`) && PathPrefix(`/notls`) kind: Rule middlewares: - name: test-auth services: - name: whoami port: 80 \u5efa\u7acb Middleware ``` apiVersion: traefik.io/v1alpha1 kind: Middleware metadata: name: test-auth spec: basicAuth: secret: authsecret Note: in a kubernetes secret the string (e.g. generated by htpasswd) must be base64-encoded first. To create an encoded user:password pair, the following command can be used: htpasswd -nb user password | openssl base64 apiVersion: v1 kind: Secret metadata: name: authsecret namespace: default data: users: dXNlcjokYXByMSRicC5FV295eCRXTWttUktBLzdjQWcycXNIL09XbXIxCgo= ``` htpasswd -nb user password | openssl base64 \u53bb\u770b Traefik Dashboard\uff0c\u6703\u770b\u5230\u591a\u4e00\u500b Middlewares \u6253\u958b\u700f\u89bd\u5668\uff0c\u8f38\u5165 http://192.168.0.23:8000/notls \u8f38\u5165\uff1auser/password \u5f8c\uff0c\u5c31\u53ef\u4ee5\u767b\u5165\u6210\u529f\uff0c\u4e26\u53d6\u5f97\u8cc7\u8a0a Middleware - Error \u5efa\u7acb\u4e00\u500b Service apiVersion: v1 kind: Service metadata: name: traefikerror spec: type: LoadBalancer ports: - name: http targetPort: 80 port: 80 selector: app: traefikerror --- apiVersion: apps/v1 kind: Deployment metadata: name: traefikerror spec: replicas: 1 selector: matchLabels: app: traefikerror template: metadata: labels: app: traefikerror spec: containers: - name: traefikerror image: guillaumebriday/traefik-custom-error-pages ports: - containerPort: 80 \u5efa\u7acb Middleware apiVersion: traefik.io/v1alpha1 kind: Middleware metadata: name: test-errors spec: errors: status: - \"400-599\" query: /{status}.html service: name: traefikerror port: 80 \u52a0\u5165 Middleware apiVersion: traefik.io/v1alpha1 kind: IngressRoute metadata: name: simpleingressroute namespace: default spec: entryPoints: - web routes: - match: Host(`192.168.0.23`) && PathPrefix(`/notls`) kind: Rule middlewares: - name: test-auth - name: test-errors services: - name: whoami port: 80 \u66b4\u9732 HTTPS \u670d\u52d9 \u5148\u628a STAR_mic.com.tw.crt \u548c STAR_mic.com.tw.key \u5169\u500b\u6a94\u6848\u653e\u5230\u57f7\u884c\u76ee\u9304\u4e0b ![Alt text](image-15.png) \u57f7\u884c\u6307\u4ee4 kubectl create secret tls mic-tls --cert=STAR_mic.com.tw.crt --key=STAR_mic.com.tw.key \u5728 IngressRoute \u4e2d\u52a0\u5165 apiVersion: traefik.io/v1alpha1 kind: IngressRoute metadata: name: ingressroutetls namespace: default spec: entryPoints: - websecure routes: - kind: Rule match: Host(`localhost.mic.com.tw`) && PathPrefix(`/tls`) services: - name: whoami port: 80 tls: secretName: mic-tls ps: \u5728\u672c\u6a5f\u7684hosts\u6a94\u4ffa\u4e2d\u52a0\u5165 192.168.0.23 localhost.mic.com.tw (192.168.0.2 \u662f\u5b58\u53d6 Traefik \u7684\u66b4\u9732IP) https://localhost.mic.com.tw/tls \u53c3\u8003\u7db2\u9801\uff1a https://www.readfog.com/a/1665375657830486016","title":"Treafik"},{"location":"traefik/#traefik","text":"","title":"Traefik"},{"location":"traefik/#with-k8s-crd-custom-resource-definitions","text":"IngressRoute Definition # Install Traefik Resource Definitions: kubectl apply -f https://raw.githubusercontent.com/traefik/traefik/v2.10/docs/content/reference/dynamic-configuration/kubernetes-crd-definition-v1.yml # Install RBAC for Traefik: kubectl apply -f https://raw.githubusercontent.com/traefik/traefik/v2.10/docs/content/reference/dynamic-configuration/kubernetes-crd-rbac.yml Services kubectl apply -f https://raw.githubusercontent.com/traefik/traefik/v2.10/docs/content/user-guides/crd-acme/02-services.yml \u4fee\u6539 traefik apiVersion: v1 kind: Service metadata: name: traefik spec: type: LoadBalancer # \u591a\u52a0\u9019\u4e00\u884c\uff0c\u66b4\u9732\u8857\u53e3 ports: - protocol: TCP name: web # web \u5165\u53e3 port: 8000 - protocol: TCP name: admin # dashboard \u5165\u53e3 port: 8080 - protocol: TCP name: websecure # web https \u5165\u53e3 port: 4443 selector: app: traefik - Deployments kubectl apply -f https://raw.githubusercontent.com/traefik/traefik/v2.10/docs/content/user-guides/crd-acme/03-deployments.yml Traefik Routers ``` apiVersion: traefik.io/v1alpha1 kind: IngressRoute metadata: name: simpleingressroute namespace: default spec: entryPoints: - web routes: match: Host( 192.168.0.17 ) && PathPrefix( /notls ) kind: Rule services: name: whoami port: 80 ``` apiVersion: traefik.io/v1alpha1 kind: IngressRoute metadata: name: docker-demo namespace: default spec: entryPoints: - web routes: - match: PathPrefix(`/docker-demo`) kind: Rule services: - name: myapp port: 80 Dashboard http://192.168.0.23:8080/dashboard/#/ \u6e2c\u8a66 URL\uff1a http://192.168.0.23:8000/notls Middlewares \u6982\u5ff5 \u4fee\u6539 IngressRoute apiVersion: traefik.io/v1alpha1 kind: IngressRoute metadata: name: simpleingressroute namespace: default spec: entryPoints: - web routes: - match: Host(`192.168.0.23`) && PathPrefix(`/notls`) kind: Rule middlewares: - name: test-auth services: - name: whoami port: 80 \u5efa\u7acb Middleware ``` apiVersion: traefik.io/v1alpha1 kind: Middleware metadata: name: test-auth spec: basicAuth: secret: authsecret","title":"With K8S + CRD (Custom Resource Definitions)"},{"location":"traefik/#note-in-a-kubernetes-secret-the-string-eg-generated-by-htpasswd-must-be-base64-encoded-first","text":"","title":"Note: in a kubernetes secret the string (e.g. generated by htpasswd) must be base64-encoded first."},{"location":"traefik/#to-create-an-encoded-userpassword-pair-the-following-command-can-be-used","text":"","title":"To create an encoded user:password pair, the following command can be used:"},{"location":"traefik/#htpasswd-nb-user-password-openssl-base64","text":"apiVersion: v1 kind: Secret metadata: name: authsecret namespace: default data: users: dXNlcjokYXByMSRicC5FV295eCRXTWttUktBLzdjQWcycXNIL09XbXIxCgo= ``` htpasswd -nb user password | openssl base64 \u53bb\u770b Traefik Dashboard\uff0c\u6703\u770b\u5230\u591a\u4e00\u500b Middlewares \u6253\u958b\u700f\u89bd\u5668\uff0c\u8f38\u5165 http://192.168.0.23:8000/notls \u8f38\u5165\uff1auser/password \u5f8c\uff0c\u5c31\u53ef\u4ee5\u767b\u5165\u6210\u529f\uff0c\u4e26\u53d6\u5f97\u8cc7\u8a0a","title":"htpasswd -nb user password | openssl base64"},{"location":"traefik/#middleware-error","text":"\u5efa\u7acb\u4e00\u500b Service apiVersion: v1 kind: Service metadata: name: traefikerror spec: type: LoadBalancer ports: - name: http targetPort: 80 port: 80 selector: app: traefikerror --- apiVersion: apps/v1 kind: Deployment metadata: name: traefikerror spec: replicas: 1 selector: matchLabels: app: traefikerror template: metadata: labels: app: traefikerror spec: containers: - name: traefikerror image: guillaumebriday/traefik-custom-error-pages ports: - containerPort: 80 \u5efa\u7acb Middleware apiVersion: traefik.io/v1alpha1 kind: Middleware metadata: name: test-errors spec: errors: status: - \"400-599\" query: /{status}.html service: name: traefikerror port: 80 \u52a0\u5165 Middleware apiVersion: traefik.io/v1alpha1 kind: IngressRoute metadata: name: simpleingressroute namespace: default spec: entryPoints: - web routes: - match: Host(`192.168.0.23`) && PathPrefix(`/notls`) kind: Rule middlewares: - name: test-auth - name: test-errors services: - name: whoami port: 80","title":"Middleware - Error"},{"location":"traefik/#https","text":"\u5148\u628a STAR_mic.com.tw.crt \u548c STAR_mic.com.tw.key \u5169\u500b\u6a94\u6848\u653e\u5230\u57f7\u884c\u76ee\u9304\u4e0b ![Alt text](image-15.png) \u57f7\u884c\u6307\u4ee4 kubectl create secret tls mic-tls --cert=STAR_mic.com.tw.crt --key=STAR_mic.com.tw.key \u5728 IngressRoute \u4e2d\u52a0\u5165 apiVersion: traefik.io/v1alpha1 kind: IngressRoute metadata: name: ingressroutetls namespace: default spec: entryPoints: - websecure routes: - kind: Rule match: Host(`localhost.mic.com.tw`) && PathPrefix(`/tls`) services: - name: whoami port: 80 tls: secretName: mic-tls ps: \u5728\u672c\u6a5f\u7684hosts\u6a94\u4ffa\u4e2d\u52a0\u5165 192.168.0.23 localhost.mic.com.tw (192.168.0.2 \u662f\u5b58\u53d6 Traefik \u7684\u66b4\u9732IP) https://localhost.mic.com.tw/tls \u53c3\u8003\u7db2\u9801\uff1a https://www.readfog.com/a/1665375657830486016","title":"\u66b4\u9732 HTTPS \u670d\u52d9"},{"location":"velero/","text":"\u53c3\u8003\u8cc7\u6599\uff1a - Velero \u521d\u63a2\u8207\u5be6\u8e10 https://kaichu.io/posts/velero-research-practice/ - \u5b98\u7db2 https://velero.io/docs/v1.11/how-velero-works/ - \u4f7f\u7528 Velero \u5099\u4efd\u9084\u539f Kubernetes \u96c6\u7fa3 https://www.readfog.com/a/1647215683490123776 - Quick start evaluation install with Minio https://velero.io/docs/main/contributions/minio - k8s1.24 \u4f7f\u7528Velero \u5099\u4efd\u9084\u539fRancher Longhorn\u4e0avolume\u8cc7\u6599 https://www.itnotetk.com/2022/11/28/k8s1-24-velero-%e5%82%99%e4%bb%bd%e9%82%84%e5%8e%9f-rancher-longhorn%e4%b8%8avolume%e8%b3%87%e6%96%99/ \u904b\u4f5c\u65b9\u5f0f Velero \u7684\u57fa\u672c\u64cd\u4f5c\u5c31\u662f CLI \u6703\u53bb\u64cd\u4f5c Kubernetes API \u5efa\u7acb Backup \u7269\u4ef6 BackupController \u5075\u6e2c\u5230\u65b0\u7684 Backup \u7269\u4ef6\u4e26\u6aa2\u67e5 \u6aa2\u67e5\u901a\u904e\u5f8c\u5c31\u6703\u64cd\u4f5c Kubernetes API Server \u9032\u884c\u8cc7\u6599\u7684\u5099\u4efd BackupController \u5c31\u6703\u900f\u904e Plugin \u6703\u64cd\u4f5c\u5c0d\u61c9\u7528 Object Storage Service \u4e0a\u50b3\u6a94\u6848 \u5982\u679c Provider \u652f\u63f4\u539f\u751f\u7684\u5feb\u7167\u64cd\u4f5c, Plugin \u5c31\u53ef\u4ee5\u900f\u904e API \u5099\u5206\u6c38\u4e45\u78c1\u789f\u5340 \u5b89\u88dd \u4e0b\u8f09\u53ca\u89e3\u58d3\u7e2e wget https://github.com/vmware-tanzu/velero/releases/download/v1.11.0/velero-v1.11.0-linux-amd64.tar.gz tar -zxvf velero-v1.11.0-linux-amd64.tar.gz && cd velero-v1.11.0-linux-amd64 \u5b89\u88ddMINIO \u8981\u5c07 examples/minio/00-minio-deployment.yaml \u4fee\u6539\u6210\u4ee5\u4e0b # Copyright 2017 the Velero contributors. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. --- apiVersion: v1 kind: Namespace metadata: name: velero --- apiVersion: apps/v1 kind: Deployment metadata: namespace: velero name: minio labels: component: minio spec: strategy: type: Recreate selector: matchLabels: component: minio template: metadata: labels: component: minio spec: volumes: - name: storage emptyDir: {} - name: config emptyDir: {} containers: - name: minio image: minio/minio:latest imagePullPolicy: IfNotPresent args: - server - /storage - --config-dir=/config - --console-address=:9001 env: - name: MINIO_ACCESS_KEY value: \"minio\" - name: MINIO_SECRET_KEY value: \"minio123\" ports: - containerPort: 9000 - containerPort: 9001 volumeMounts: - name: storage mountPath: \"/storage\" - name: config mountPath: \"/config\" --- apiVersion: v1 kind: Service metadata: namespace: velero name: minio labels: component: minio spec: type: NodePort ports: - name: api port: 9000 targetPort: 9000 - name: console port: 9001 targetPort: 9001 selector: component: minio --- apiVersion: batch/v1 kind: Job metadata: namespace: velero name: minio-setup labels: component: minio spec: template: metadata: name: minio-setup spec: restartPolicy: OnFailure volumes: - name: config emptyDir: {} containers: - name: mc image: minio/mc:latest imagePullPolicy: IfNotPresent command: - /bin/sh - -c - \"mc --config-dir=/config config host add velero http://minio:9000 minio minio123 && mc --config-dir=/config mb -p velero/velero\" volumeMounts: - name: config mountPath: \"/config\" kubectl apply -f examples/minio/00-minio-deployment.yaml \u958b\u555f kubrnetes dashboard \u770b velero minio \u662f\u54ea\u500b Port\uff0c\u4e26\u958b\u555f\u700f\u89bd\u5668 http://192.168.0.17:32729/browser \u5e33\u865f/\u5bc6\u78bc\uff1aminio/minio123 P.S.\uff1a \u7576\u7136\u5982\u679c\u9700\u8981\u5728\u4e0d\u540c Kubernetes \u548c\u5b58\u5132\u6c60\u96c6\u7fa3\u5099\u4efd\u8207\u6062\u5fa9\u6578\u64da\uff0c\u9700\u8981\u5c07 minio \u670d\u52d9\u7aef\u5b89\u88dd\u5728 Kubernetes \u96c6\u7fa3\u5916\uff0c\u4fdd\u8b49\u5728\u96c6\u7fa3\u767c\u751f\u707d\u96e3\u6027\u6545\u969c\u6642\uff0c\u4e0d\u6703\u5c0d\u5099\u4efd\u6578\u64da\u7522\u751f\u5f71\u97ff\uff0c\u53ef\u4ee5\u901a\u904e\u4e8c\u9032\u5236\u7684\u65b9\u5f0f\u9032\u884c\u5b89\u88dd\u3002 (\u7565) \u8acb\u53c3\u8003 https://www.readfog.com/a/1647215683490123776 \u5b89\u88dd MINIO \u5b89\u88dd velero \u670d\u52d9\u7aef nano credentials-velero # \u79d8\u94a5\u6587\u4ef6credentials-velero [default] aws_access_key_id = minio aws_secret_access_key = minio123 velero install velero install \\ --provider aws \\ --bucket velero \\ --plugins velero/velero-plugin-for-aws:latest \\ --plugins openebs/velero-plugin:ci \\ --secret-file ./credentials-velero \\ --use-volume-snapshots=false \\ --backup-location-config region=minio,s3ForcePathStyle=\"true\",s3Url=http://minio.velero.svc.cluster.local:9000 PS\uff1as3Url=http://192.168.0.17:31883 PROT \u5f9e\u54ea\u88e1\u53d6\u5f97\uff1f minio.velero:30050 TCP --> MiniO \u7684 URL minio.velero:31883 TCP --> MiniO \u7684 Dashboard URL http://192.168.0.17:30050/ Back up \u5148\u5efa\u7acb\u4e00\u500b Namespace / Development / Service cd ~/velero-v1.11.0-linux-amd64 kubectl apply -f examples/nginx-app/base.yaml Create a backup for any object that matches the app=nginx label selector: velero backup create nginx-backup --selector app=nginx Run velero backup describe nginx-backup or velero backup logs nginx-backup for more details. \u9019\u6642\u5019\u53bb\u770b MiniO \u7684 Dashboard\uff0c\u6703\u770b\u5230\u5099\u4efd\u7684\u6a94\u6848\u5728\u9019\u3002 Simulate a disaster kubectl delete namespace nginx-example To check that the nginx deployment and service are gone, run: kubectl get deployments --namespace=nginx-example kubectl get services --namespace=nginx-example kubectl get namespace/nginx-example Restore velero restore create --from-backup nginx-backup Restore request \"nginx-backup-20230710100823\" submitted successfully. Run velero restore describe nginx-backup-20230710100823 or velero restore logs nginx-backup-20230710100823 for more details. # \u770b\u9032\u5ea6 velero restore get nginx-example \u5c31\u56de\u4f86\u4e86~ Clean up velero backup delete BACKUP_NAME velero uninstall --> \u5168\u90e8\u79fb\u9664 With PV \u7bc4\u4f8b1 cd ~/velero-v1.11.0-linux-amd64 kubectl apply -f examples/nginx-app/with-pv.yaml kubectl -n nginx-example annotate pod/nginx-deployment-78964c9995-cx4qs backup.velero.io/backup-volumes=nginx-logs velero backup create nginx-backup --include-namespaces nginx-example --default-volumes-to-fs-backup --snapshot-volumes --ttl 180h velero backup create nginx-backup --include-namespaces nginx-example --default-volumes-to-fs-backup \u7bc4\u4f8b2 \u8acb\u53c3\u8003 nfs \u7684 MySQL + NFS \u5efa\u7acb\u76f8\u95dc\u74b0\u5883\u8207pod kubectl exec -it mysql-5b46fb64b4-t2m4c -n kube-demo -- /bin/bash \u4e26\u4e14\u5efa\u7acb\u4e00\u500b\u65b0\u7684\u8cc7\u6599\u5eab\uff0c\u4f5c\u70ba\u6e2c\u8a66\u7528 \u5099\u4efd velero backup create mysql-backup --include-namespaces kube-demo --default-volumes-to-restic velero backup describe mysql-backup minio \u6703\u6709\u5099\u4efd\u8cc7\u6599 ![Alt text](image-49.png) \u6a21\u64ec\u707d\u96e3 kubectl delete namespace kube-demo \u5fa9\u539f velero restore create --from-backup mysql-backup velero restore get \u6aa2\u67e5\u8cc7\u6599\u6709\u5426\u9084\u539f kubectl exec -it mysql-5b46fb64b4-zj7n2 -n kube-demo -- /bin/bash","title":"Velero"},{"location":"velero/#_1","text":"Velero \u7684\u57fa\u672c\u64cd\u4f5c\u5c31\u662f CLI \u6703\u53bb\u64cd\u4f5c Kubernetes API \u5efa\u7acb Backup \u7269\u4ef6 BackupController \u5075\u6e2c\u5230\u65b0\u7684 Backup \u7269\u4ef6\u4e26\u6aa2\u67e5 \u6aa2\u67e5\u901a\u904e\u5f8c\u5c31\u6703\u64cd\u4f5c Kubernetes API Server \u9032\u884c\u8cc7\u6599\u7684\u5099\u4efd BackupController \u5c31\u6703\u900f\u904e Plugin \u6703\u64cd\u4f5c\u5c0d\u61c9\u7528 Object Storage Service \u4e0a\u50b3\u6a94\u6848 \u5982\u679c Provider \u652f\u63f4\u539f\u751f\u7684\u5feb\u7167\u64cd\u4f5c, Plugin \u5c31\u53ef\u4ee5\u900f\u904e API \u5099\u5206\u6c38\u4e45\u78c1\u789f\u5340","title":"\u904b\u4f5c\u65b9\u5f0f"},{"location":"velero/#_2","text":"\u4e0b\u8f09\u53ca\u89e3\u58d3\u7e2e wget https://github.com/vmware-tanzu/velero/releases/download/v1.11.0/velero-v1.11.0-linux-amd64.tar.gz tar -zxvf velero-v1.11.0-linux-amd64.tar.gz && cd velero-v1.11.0-linux-amd64","title":"\u5b89\u88dd"},{"location":"velero/#minio","text":"\u8981\u5c07 examples/minio/00-minio-deployment.yaml \u4fee\u6539\u6210\u4ee5\u4e0b # Copyright 2017 the Velero contributors. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. --- apiVersion: v1 kind: Namespace metadata: name: velero --- apiVersion: apps/v1 kind: Deployment metadata: namespace: velero name: minio labels: component: minio spec: strategy: type: Recreate selector: matchLabels: component: minio template: metadata: labels: component: minio spec: volumes: - name: storage emptyDir: {} - name: config emptyDir: {} containers: - name: minio image: minio/minio:latest imagePullPolicy: IfNotPresent args: - server - /storage - --config-dir=/config - --console-address=:9001 env: - name: MINIO_ACCESS_KEY value: \"minio\" - name: MINIO_SECRET_KEY value: \"minio123\" ports: - containerPort: 9000 - containerPort: 9001 volumeMounts: - name: storage mountPath: \"/storage\" - name: config mountPath: \"/config\" --- apiVersion: v1 kind: Service metadata: namespace: velero name: minio labels: component: minio spec: type: NodePort ports: - name: api port: 9000 targetPort: 9000 - name: console port: 9001 targetPort: 9001 selector: component: minio --- apiVersion: batch/v1 kind: Job metadata: namespace: velero name: minio-setup labels: component: minio spec: template: metadata: name: minio-setup spec: restartPolicy: OnFailure volumes: - name: config emptyDir: {} containers: - name: mc image: minio/mc:latest imagePullPolicy: IfNotPresent command: - /bin/sh - -c - \"mc --config-dir=/config config host add velero http://minio:9000 minio minio123 && mc --config-dir=/config mb -p velero/velero\" volumeMounts: - name: config mountPath: \"/config\" kubectl apply -f examples/minio/00-minio-deployment.yaml \u958b\u555f kubrnetes dashboard \u770b velero minio \u662f\u54ea\u500b Port\uff0c\u4e26\u958b\u555f\u700f\u89bd\u5668 http://192.168.0.17:32729/browser \u5e33\u865f/\u5bc6\u78bc\uff1aminio/minio123 P.S.\uff1a \u7576\u7136\u5982\u679c\u9700\u8981\u5728\u4e0d\u540c Kubernetes \u548c\u5b58\u5132\u6c60\u96c6\u7fa3\u5099\u4efd\u8207\u6062\u5fa9\u6578\u64da\uff0c\u9700\u8981\u5c07 minio \u670d\u52d9\u7aef\u5b89\u88dd\u5728 Kubernetes \u96c6\u7fa3\u5916\uff0c\u4fdd\u8b49\u5728\u96c6\u7fa3\u767c\u751f\u707d\u96e3\u6027\u6545\u969c\u6642\uff0c\u4e0d\u6703\u5c0d\u5099\u4efd\u6578\u64da\u7522\u751f\u5f71\u97ff\uff0c\u53ef\u4ee5\u901a\u904e\u4e8c\u9032\u5236\u7684\u65b9\u5f0f\u9032\u884c\u5b89\u88dd\u3002 (\u7565) \u8acb\u53c3\u8003 https://www.readfog.com/a/1647215683490123776 \u5b89\u88dd MINIO","title":"\u5b89\u88ddMINIO"},{"location":"velero/#velero","text":"nano credentials-velero # \u79d8\u94a5\u6587\u4ef6credentials-velero [default] aws_access_key_id = minio aws_secret_access_key = minio123 velero install velero install \\ --provider aws \\ --bucket velero \\ --plugins velero/velero-plugin-for-aws:latest \\ --plugins openebs/velero-plugin:ci \\ --secret-file ./credentials-velero \\ --use-volume-snapshots=false \\ --backup-location-config region=minio,s3ForcePathStyle=\"true\",s3Url=http://minio.velero.svc.cluster.local:9000 PS\uff1as3Url=http://192.168.0.17:31883 PROT \u5f9e\u54ea\u88e1\u53d6\u5f97\uff1f minio.velero:30050 TCP --> MiniO \u7684 URL minio.velero:31883 TCP --> MiniO \u7684 Dashboard URL http://192.168.0.17:30050/","title":"\u5b89\u88dd velero \u670d\u52d9\u7aef"},{"location":"velero/#back-up","text":"\u5148\u5efa\u7acb\u4e00\u500b Namespace / Development / Service cd ~/velero-v1.11.0-linux-amd64 kubectl apply -f examples/nginx-app/base.yaml Create a backup for any object that matches the app=nginx label selector: velero backup create nginx-backup --selector app=nginx Run velero backup describe nginx-backup or velero backup logs nginx-backup for more details. \u9019\u6642\u5019\u53bb\u770b MiniO \u7684 Dashboard\uff0c\u6703\u770b\u5230\u5099\u4efd\u7684\u6a94\u6848\u5728\u9019\u3002 Simulate a disaster kubectl delete namespace nginx-example To check that the nginx deployment and service are gone, run: kubectl get deployments --namespace=nginx-example kubectl get services --namespace=nginx-example kubectl get namespace/nginx-example","title":"Back up"},{"location":"velero/#restore","text":"velero restore create --from-backup nginx-backup Restore request \"nginx-backup-20230710100823\" submitted successfully. Run velero restore describe nginx-backup-20230710100823 or velero restore logs nginx-backup-20230710100823 for more details. # \u770b\u9032\u5ea6 velero restore get nginx-example \u5c31\u56de\u4f86\u4e86~","title":"Restore"},{"location":"velero/#clean-up","text":"velero backup delete BACKUP_NAME velero uninstall --> \u5168\u90e8\u79fb\u9664","title":"Clean up"},{"location":"velero/#with-pv","text":"\u7bc4\u4f8b1 cd ~/velero-v1.11.0-linux-amd64 kubectl apply -f examples/nginx-app/with-pv.yaml kubectl -n nginx-example annotate pod/nginx-deployment-78964c9995-cx4qs backup.velero.io/backup-volumes=nginx-logs velero backup create nginx-backup --include-namespaces nginx-example --default-volumes-to-fs-backup --snapshot-volumes --ttl 180h velero backup create nginx-backup --include-namespaces nginx-example --default-volumes-to-fs-backup \u7bc4\u4f8b2 \u8acb\u53c3\u8003 nfs \u7684 MySQL + NFS \u5efa\u7acb\u76f8\u95dc\u74b0\u5883\u8207pod kubectl exec -it mysql-5b46fb64b4-t2m4c -n kube-demo -- /bin/bash \u4e26\u4e14\u5efa\u7acb\u4e00\u500b\u65b0\u7684\u8cc7\u6599\u5eab\uff0c\u4f5c\u70ba\u6e2c\u8a66\u7528 \u5099\u4efd velero backup create mysql-backup --include-namespaces kube-demo --default-volumes-to-restic velero backup describe mysql-backup minio \u6703\u6709\u5099\u4efd\u8cc7\u6599 ![Alt text](image-49.png) \u6a21\u64ec\u707d\u96e3 kubectl delete namespace kube-demo \u5fa9\u539f velero restore create --from-backup mysql-backup velero restore get \u6aa2\u67e5\u8cc7\u6599\u6709\u5426\u9084\u539f kubectl exec -it mysql-5b46fb64b4-zj7n2 -n kube-demo -- /bin/bash","title":"With PV"}]}